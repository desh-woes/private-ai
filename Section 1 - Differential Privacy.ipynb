{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Toy Differential Privacy - Simple Database Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we're going to play around with Differential Privacy in the context of a database query. The database is going to be a VERY simple database with only one boolean column. Each row corresponds to a person. Each value corresponds to whether or not that person has a certain private attribute (such as whether they have a certain disease, or whether they are above/below a certain age). We are then going to learn how to know whether a database query over such a small database is differentially private or not - and more importantly - what techniques are at our disposal to ensure various levels of privacy\n",
    "\n",
    "\n",
    "### First We Create a Simple Database\n",
    "\n",
    "Step one is to create our database - we're going to do this by initializing a random list of 1s and 0s (which are the entries in our database). Note - the number of entries directly corresponds to the number of people in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 1, 1, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# the number of entries in our database\n",
    "num_entries = 5000\n",
    "\n",
    "db = torch.rand(num_entries) > 0.5\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Generate Parallel Databases\n",
    "\n",
    "Key to the definition of differenital privacy is the ability to ask the question \"When querying a database, if I removed someone from the database, would the output of the query be any different?\". Thus, in order to check this, we must construct what we term \"parallel databases\" which are simply databases with one entry removed. \n",
    "\n",
    "In this first project, I want you to create a list of every parallel database to the one currently contained in the \"db\" variable. Then, I want you to create a function which both:\n",
    "\n",
    "- creates the initial database (db)\n",
    "- creates all parallel databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try project here!\n",
    "def gen_parallel_db(data_base, index):\n",
    "    return torch.cat((data_base[0:index], data_base[index+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_all_parallel_db(data_base):\n",
    "    parallel_dbs = []\n",
    "    for i in range(len(data_base)):\n",
    "        parallel_dbs.append(gen_parallel_db(data_base, i))\n",
    "    return parallel_dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_db_and_parallels(num_elements):\n",
    "    db = torch.rand(num_elements) > 0.5\n",
    "    return db, gen_all_parallel_db(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 0, 1], dtype=torch.uint8),\n",
       " [tensor([1, 0, 1], dtype=torch.uint8),\n",
       "  tensor([0, 0, 1], dtype=torch.uint8),\n",
       "  tensor([0, 1, 1], dtype=torch.uint8),\n",
       "  tensor([0, 1, 0], dtype=torch.uint8)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_db_and_parallels(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Towards Evaluating The Differential Privacy of a Function\n",
    "\n",
    "Intuitively, we want to be able to query our database and evaluate whether or not the result of the query is leaking \"private\" information. As mentioned previously, this is about evaluating whether the output of a query changes when we remove someone from the database. Specifically, we want to evaluate the *maximum* amount the query changes when someone is removed (maximum over all possible people who could be removed). So, in order to evaluate how much privacy is leaked, we're going to iterate over each person in the database and measure the difference in the output of the query relative to when we query the entire database. \n",
    "\n",
    "Just for the sake of argument, let's make our first \"database query\" a simple sum. Aka, we're going to count the number of 1s in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, pdbs = gen_db_and_parallels(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db):\n",
    "    return db.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_db_result = query(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0\n",
    "for pdb in pdbs:\n",
    "    pdb_result = query(pdb)\n",
    "    \n",
    "    db_distance = torch.abs(pdb_result - full_db_result)\n",
    "    \n",
    "    if(db_distance > sensitivity):\n",
    "        sensitivity = db_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Evaluating the Privacy of a Function\n",
    "\n",
    "In the last section, we measured the difference between each parallel db's query result and the query result for the entire database and then calculated the max value (which was 1). This value is called \"sensitivity\", and it corresponds to the function we chose for the query. Namely, the \"sum\" query will always have a sensitivity of exactly 1. However, we can also calculate sensitivity for other functions as well.\n",
    "\n",
    "Let's try to calculate sensitivity for the \"mean\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!\n",
    "def sensitivity(query, n_entries):\n",
    "    # Initialize the database\n",
    "    database, parallel_databases = gen_db_and_parallels(n_entries)\n",
    "    \n",
    "    # Run query over all the databases\n",
    "    full_db_result = query(database)\n",
    "    sensitivity = 0\n",
    "    \n",
    "    for db in parallel_databases:\n",
    "        pdb_result = query(db)\n",
    "        distance = torch.abs(pdb_result-full_db_result)\n",
    "    \n",
    "        if distance > sensitivity:\n",
    "            sensitivity = distance\n",
    "    return sensitivity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "def query(db):\n",
    "    return db.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity(query, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That sensitivity is WAY lower. Note the intuition here. \"Sensitivity\" is measuring how sensitive the output of the query is to a person being removed from the database. For a simple sum, this is always 1, but for the mean, removing a person is going to change the result of the query by rougly 1 divided by the size of the database (which is much smaller). Thus, \"mean\" is a VASTLY less \"sensitive\" function (query) than SUM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Calculate L1 Sensitivity For Threshold\n",
    "\n",
    "In this first project, I want you to calculate the sensitivty for the \"threshold\" function. \n",
    "\n",
    "- First compute the sum over the database (i.e. sum(db)) and return whether that sum is greater than a certain threshold.\n",
    "- Then, I want you to create databases of size 10 and threshold of 5 and calculate the sensitivity of the function. \n",
    "- Finally, re-initialize the database 10 times and calculate the sensitivity each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!\n",
    "def threshold(database, threshold_val = 5):\n",
    "    return (database.sum() > threshold_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity_arr = [10 for i in range(10)]\n",
    "sensitivity_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, tensor(1.), 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in range(len(sensitivity_arr)):\n",
    "    sensitivity_arr[index] = sensitivity(threshold, sensitivity_arr[index])\n",
    "sensitivity_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: A Basic Differencing Attack\n",
    "\n",
    "Sadly none of the functions we've looked at so far are differentially private (despite them having varying levels of sensitivity). The most basic type of attack can be done as follows.\n",
    "\n",
    "Let's say we wanted to figure out a specific person's value in the database. All we would have to do is query for the sum of the entire database and then the sum of the entire database without that person!\n",
    "\n",
    "# Project: Perform a Differencing Attack on Row 10\n",
    "\n",
    "In this project, I want you to construct a database and then demonstrate how you can use two different sum queries to explose the value of the person represented by row 10 in the database (note, you'll need to use a database with at least 10 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database with at least ten rows\n",
    "database = torch.rand(20) > 0.5\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sum = database.sum()\n",
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_sum = torch.cat((database[0:10], database[11:])).sum()\n",
    "partial_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Local Differential Privacy\n",
    "\n",
    "As you can see, the basic sum query is not differentially private at all! In truth, differential privacy always requires a form of randomness added to the query. Let me show you what I mean.\n",
    "\n",
    "### Randomized Response (Local Differential Privacy)\n",
    "\n",
    "Let's say I have a group of people I wish to survey about a very taboo behavior which I think they will lie about (say, I want to know if they have ever committed a certain kind of crime). I'm not a policeman, I'm just trying to collect statistics to understand the higher level trend in society. So, how do we do this? One technique is to add randomness to each person's response by giving each person the following instructions (assuming I'm asking a simple yes/no question):\n",
    "\n",
    "- Flip a coin 2 times.\n",
    "- If the first coin flip is heads, answer honestly\n",
    "- If the first coin flip is tails, answer according to the second coin flip (heads for yes, tails for no)!\n",
    "\n",
    "Thus, each person is now protected with \"plausible deniability\". If they answer \"Yes\" to the question \"have you committed X crime?\", then it might becasue they actually did, or it might be becasue they are answering according to a random coin flip. Each person has a high degree of protection. Furthermore, we can recover the underlying statistics with some accuracy, as the \"true statistics\" are simply averaged with a 50% probability. Thus, if we collect a bunch of samples and it turns out that 60% of people answer yes, then we know that the TRUE distribution is actually centered around 70%, because 70% averaged wtih 50% (a coin flip) is 60% which is the result we obtained. \n",
    "\n",
    "However, it should be noted that, especially when we only have a few samples, the this comes at the cost of accuracy. This tradeoff exists across all of Differential Privacy. The greater the privacy protection (plausible deniability) the less accurate the results. \n",
    "\n",
    "Let's implement this local DP for our database before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Result:  tensor(0.5220)\n",
      "Randomised Result:  tensor(0.5300)\n"
     ]
    }
   ],
   "source": [
    "# try this project here!\n",
    "# Approach 1\n",
    "import random\n",
    "\n",
    "def local_diff_priv(query, len_db):\n",
    "    # Generate the database\n",
    "    database = torch.rand(len_db)>0.5\n",
    "    \n",
    "    # Get the result of the query on the initial database\n",
    "    original_result = query(database)\n",
    "    \n",
    "    # Randomise the database to allow for plausible denyability\n",
    "    for index, entry in enumerate(database):\n",
    "        # Generate the random coin flips\n",
    "        coin_flip1, coin_flip2 = random.randint(0, 1), random.randint(0, 1)\n",
    "        \n",
    "        # Check if the first flip is a not head\n",
    "        if coin_flip1 == 1:\n",
    "            continue\n",
    "        else:\n",
    "            database[index] = coin_flip2\n",
    "            \n",
    "    # Now generate a new randomised result.\n",
    "    randomised_result = query(database) *2 - 0.5\n",
    "    \n",
    "    # Return the original result and the randomised result\n",
    "    return original_result, randomised_result  \n",
    "\n",
    "# Define query function\n",
    "def query(data_base):\n",
    "    return data_base.float().mean()\n",
    "\n",
    "# Test output for different database sizes\n",
    "original_res, randomised_res = local_diff_priv(query, 1000)\n",
    "print(\"Original Result: \", original_res)\n",
    "print(\"Randomised Result: \", randomised_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 2\n",
    "database, paralle_database = gen_db_and_parallels(100)\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_result = database.float().mean()\n",
    "true_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_coin_flip = (torch.rand(len(database))>0.5).float()\n",
    "second_coin_flip = (torch.rand(len(database))>0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.float() * first_coin_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- first_coin_flip)*second_coin_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_database = (database.float() * first_coin_flip) + ((1-first_coin_flip)*second_coin_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional approach 2\n",
    "def query(db):\n",
    "    # Calculate the true result\n",
    "    true_result = torch.mean(db.float())\n",
    "    \n",
    "    # Flip the coins and cast to float\n",
    "    first_coin_flip = (torch.rand(len(db))>0.5).float()\n",
    "    second_coin_flip = (torch.rand(len(db))>0.5).float()\n",
    "    \n",
    "    # Calculate the result of the augmented database\n",
    "    augmented_database = (db.float()*first_coin_flip) + ((1-first_coin_flip)*second_coin_flip)\n",
    "    \n",
    "    # Calculate the augmented result\n",
    "    augmented_result = augmented_database.mean()\n",
    "    \n",
    "    # remove the skew of the augmented database\n",
    "    unskewed_augmented_result = (augmented_result * 2) - 0.5\n",
    "    \n",
    "    return true_result, unskewed_augmented_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.5300), tensor(0.5540))\n"
     ]
    }
   ],
   "source": [
    "db, pdbs = gen_db_and_parallels(1000)\n",
    "print(query(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Varying Amounts of Noise\n",
    "\n",
    "In this project, I want you to augment the randomized response query (the one we just wrote) to allow for varying amounts of randomness to be added. Specifically, I want you to bias the coin flip to be higher or lower and then run the same experiment. \n",
    "\n",
    "Note - this one is a bit tricker than you might expect. You need to both adjust the likelihood of the first coin flip AND the de-skewing at the end (where we create the \"augmented_result\" variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Result:  tensor(0.5150)\n",
      "Randomised Result:  tensor(0.5160)\n"
     ]
    }
   ],
   "source": [
    "# Approach 1\n",
    "import random\n",
    "\n",
    "def local_diff_priv(query, len_db, varying_noise):\n",
    "    # Generate the database\n",
    "    database = torch.rand(len_db)>0.5\n",
    "    \n",
    "    # Get the result of the query on the initial database\n",
    "    original_result = query(database)\n",
    "    \n",
    "    # Randomise the database to allow for plausible denyability\n",
    "    for index, entry in enumerate(database):\n",
    "        # Generate the random coin flips\n",
    "        coin_flip1, coin_flip2 = random.randint(0, 1), random.randint(0, 1)\n",
    "        \n",
    "        # Check if the first flip is a not head\n",
    "        if coin_flip1 == 1:\n",
    "            continue\n",
    "        else:\n",
    "            random_number = random.random()\n",
    "            if random_number < varying_noise:\n",
    "                database[index] = coin_flip2\n",
    "            \n",
    "    # Now generate a new randomised result.\n",
    "    randomised_result = query(database) *2 - 0.5\n",
    "    \n",
    "    # Return the original result and the randomised result\n",
    "    return original_result, randomised_result  \n",
    "\n",
    "# Define query function\n",
    "def query(data_base):\n",
    "    return data_base.float().mean()\n",
    "\n",
    "# Test output for different database sizes\n",
    "original_res, randomised_res = local_diff_priv(query, 1000, 0.3)\n",
    "print(\"Original Result: \", original_res)\n",
    "print(\"Randomised Result: \", randomised_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: The Formal Definition of Differential Privacy\n",
    "\n",
    "The previous method of adding noise was called \"Local Differentail Privacy\" because we added noise to each datapoint individually. This is necessary for some situations wherein the data is SO sensitive that individuals do not trust noise to be added later. However, it comes at a very high cost in terms of accuracy. \n",
    "\n",
    "However, alternatively we can add noise AFTER data has been aggregated by a function. This kind of noise can allow for similar levels of protection with a lower affect on accuracy. However, participants must be able to trust that no-one looked at their datapoints _before_ the aggregation took place. In some situations this works out well, in others (such as an individual hand-surveying a group of people), this is less realistic.\n",
    "\n",
    "Nevertheless, global differential privacy is incredibly important because it allows us to perform differential privacy on smaller groups of individuals with lower amounts of noise. Let's revisit our sum functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db, pdbs = gen_db_and_parallels(100)\n",
    "\n",
    "def query(db):\n",
    "    return torch.sum(db.float())\n",
    "\n",
    "def M(db):\n",
    "    query(db) + noise\n",
    "\n",
    "query(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea here is that we want to add noise to the output of our function. We actually have two different kinds of noise we can add - Laplacian Noise or Gaussian Noise. However, before we do so at this point we need to dive into the formal definition of Differential Privacy.\n",
    "\n",
    "![alt text](dp_formula.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Image From: \"The Algorithmic Foundations of Differential Privacy\" - Cynthia Dwork and Aaron Roth - https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition does not _create_ differential privacy, instead it is a measure of how much privacy is afforded by a query M. Specifically, it's a comparison between running the query M on a database (x) and a parallel database (y). As you remember, parallel databases are defined to be the same as a full database (x) with one entry/person removed.\n",
    "\n",
    "Thus, this definition says that FOR ALL parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon, but that occasionally this constraint won't hold with probability delta. Thus, this theorem is called \"epsilon delta\" differential privacy.\n",
    "\n",
    "# Epsilon\n",
    "\n",
    "Let's unpack the intuition of this for a moment. \n",
    "\n",
    "Epsilon Zero: If a query satisfied this inequality where epsilon was set to 0, then that would mean that the query for all parallel databases outputed the exact same value as the full database. As you may remember, when we calculated the \"threshold\" function, often the Sensitivity was 0. In that case, the epsilon also happened to be zero.\n",
    "\n",
    "Epsilon One: If a query satisfied this inequality with epsilon 1, then the maximum distance between all queries would be 1 - or more precisely - the maximum distance between the two random distributions M(x) and M(y) is 1 (because all these queries have some amount of randomness in them, just like we observed in the last section).\n",
    "\n",
    "# Delta\n",
    "\n",
    "Delta is basically the probability that epsilon breaks. Namely, sometimes the epsilon is different for some queries than it is for others. For example, you may remember when we were calculating the sensitivity of threshold, most of the time sensitivity was 0 but sometimes it was 1. Thus, we could calculate this as \"epsilon zero but non-zero delta\" which would say that epsilon is perfect except for some probability of the time when it's arbitrarily higher. Note that this expression doesn't represent the full tradeoff between epsilon and delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: How To Add Noise for Global Differential Privacy\n",
    "\n",
    "In this lesson, we're going to learn about how to take a query and add varying amounts of noise so that it satisfies a certain degree of differential privacy. In particular, we're going to leave behind the Local Differential privacy previously discussed and instead opt to focus on Global differential privacy. \n",
    "\n",
    "So, to sum up, this lesson is about adding noise to the output of our query so that it satisfies a certain epsilon-delta differential privacy threshold.\n",
    "\n",
    "There are two kinds of noise we can add - Gaussian Noise or Laplacian Noise. Generally speaking Laplacian is better, but both are still valid. Now to the hard question...\n",
    "\n",
    "### How much noise should we add?\n",
    "\n",
    "The amount of noise necessary to add to the output of a query is a function of four things:\n",
    "\n",
    "- the type of noise (Gaussian/Laplacian)\n",
    "- the sensitivity of the query/function\n",
    "- the desired epsilon (ε)\n",
    "- the desired delta (δ)\n",
    "\n",
    "Thus, for each type of noise we're adding, we have different way of calculating how much to add as a function of sensitivity, epsilon, and delta. We're going to focus on Laplacian noise. Laplacian noise is increased/decreased according to a \"scale\" parameter b. We choose \"b\" based on the following formula.\n",
    "\n",
    "b = sensitivity(query) / epsilon\n",
    "\n",
    "In other words, if we set b to be this value, then we know that we will have a privacy leakage of <= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we'll ignore them for now.\n",
    "\n",
    "### Querying Repeatedly\n",
    "\n",
    "- if we query the database multiple times - we can simply add the epsilons (Even if we change the amount of noise and their epsilons are not the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Create a Differentially Private Query\n",
    "\n",
    "In this project, I want you to take what you learned in the previous lesson and create a query function which sums over the database and adds just the right amount of noise such that it satisfies an epsilon constraint. Write a query for both \"sum\" and for \"mean\". Ensure that you use the correct sensitivity measures for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "database, parallel_database = gen_db_and_parallels(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_query(db):\n",
    "    return db.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_mechanism(database, query, sensitivity):\n",
    "    beta = sensitivity/epsilon\n",
    "    \n",
    "    # Create a noise tensor.\n",
    "    noise = torch.tensor(np.random.laplace(0, beta, 1))\n",
    "    \n",
    "    return query(database) + noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([61.9742], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian_mechanism(database, sum_query, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Differential Privacy for Deep Learning\n",
    "\n",
    "So in the last lessons you may have been wondering - what does all of this have to do with Deep Learning? Well, these same techniques we were just studying form the core primitives for how Differential Privacy provides guarantees in the context of Deep Learning. \n",
    "\n",
    "Previously, we defined perfect privacy as \"a query to a database returns the same value even if we remove any person from the database\", and used this intuition in the description of epsilon/delta. In the context of deep learning we have a similar standard.\n",
    "\n",
    "Training a model on a dataset should return the same model even if we remove any person from the dataset.\n",
    "\n",
    "Thus, we've replaced \"querying a database\" with \"training a model on a dataset\". In essence, the training process is a kind of query. However, one should note that this adds two points of complexity which database queries did not have:\n",
    "\n",
    "    1. do we always know where \"people\" are referenced in the dataset?\n",
    "    2. neural models rarely never train to the same output model, even on identical data\n",
    "\n",
    "The answer to (1) is to treat each training example as a single, separate person. Strictly speaking, this is often overly zealous as some training examples have no relevance to people and others may have multiple/partial (consider an image with multiple people contained within it). Thus, localizing exactly where \"people\" are referenced, and thus how much your model would change if people were removed, is challenging.\n",
    "\n",
    "The answer to (2) is also an open problem - but several interesitng proposals have been made. We're going to focus on one of the most popular proposals, PATE.\n",
    "\n",
    "## An Example Scenario: A Health Neural Network\n",
    "\n",
    "First we're going to consider a scenario - you work for a hospital and you have a large collection of images about your patients. However, you don't know what's in them. You would like to use these images to develop a neural network which can automatically classify them, however since your images aren't labeled, they aren't sufficient to train a classifier. \n",
    "\n",
    "However, being a cunning strategist, you realize that you can reach out to 10 partner hospitals which DO have annotated data. It is your hope to train your new classifier on their datasets so that you can automatically label your own. While these hospitals are interested in helping, they have privacy concerns regarding information about their patients. Thus, you will use the following technique to train a classifier which protects the privacy of patients in the other hospitals.\n",
    "\n",
    "- 1) You'll ask each of the 10 hospitals to train a model on their own datasets (All of which have the same kinds of labels)\n",
    "- 2) You'll then use each of the 10 partner models to predict on your local dataset, generating 10 labels for each of your datapoints\n",
    "- 3) Then, for each local data point (now with 10 labels), you will perform a DP query to generate the final true label. This query is a \"max\" function, where \"max\" is the most frequent label across the 10 labels. We will need to add laplacian noise to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 4) Finally, we will retrain a new model on our local dataset which now has labels. This will be our final \"DP\" model.\n",
    "\n",
    "So, let's walk through these steps. I will assume you're already familiar with how to train/predict a deep neural network, so we'll skip steps 1 and 2 and work with example data. We'll focus instead on step 3, namely how to perform the DP query for each example using toy data.\n",
    "\n",
    "So, let's say we have 10,000 training examples, and we've got 10 labels for each example (from our 10 \"teacher models\" which were trained directly on private data). Each label is chosen from a set of 10 possible labels (categories) for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teachers = 10 # we're working with 10 partner hospitals\n",
    "num_examples = 10000 # the size of OUR dataset\n",
    "num_labels = 10 # number of lablels for our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2 = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int).transpose(1,0) # fake predictions\n",
    "preds2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 2, 1, 0, 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_image = preds2[0]\n",
    "label_counts = np.bincount(an_image, minlength = num_labels)\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = list()\n",
    "for an_image in preds:\n",
    "\n",
    "    label_counts = np.bincount(an_image, minlength=num_labels)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    beta = 1 / epsilon\n",
    "\n",
    "    for i in range(len(label_counts)):\n",
    "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "\n",
    "    new_label = np.argmax(label_counts)\n",
    "    \n",
    "    new_labels.append(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([9, 9, 3, 6, 9, 9, 9, 9, 8, 2])\n",
    "counts = np.bincount(labels, minlength=10)\n",
    "query_result = np.argmax(counts)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 16:15:58.432812  7392 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0718 16:15:59.303534  7392 deprecation_wrapper.py:119] From C:\\Users\\Desh\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted\\session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e602c08904ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_teachers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_teachers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fake preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# true answers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "num_teachers, num_examples, num_labels = (100, 100, 10)\n",
    "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int) #fake preds\n",
    "indices = (np.random.rand(num_examples) * num_labels).astype(int) # true answers\n",
    "\n",
    "preds[:,0:10] *= 0\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5, moments = 20)\n",
    "\n",
    "assert data_dep_eps < data_ind_eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 0.9029013677789843\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5, moments = 20)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0:50] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 0.9029013677789843\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5, moments=20)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to Go From Here\n",
    "\n",
    "\n",
    "Read:\n",
    "    - Algorithmic Foundations of Differential Privacy: https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\n",
    "    - Deep Learning with Differential Privacy: https://arxiv.org/pdf/1607.00133.pdf\n",
    "    - The Ethical Algorithm: https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205\n",
    "   \n",
    "Topics:\n",
    "    - The Exponential Mechanism\n",
    "    - The Moment's Accountant\n",
    "    - Differentially Private Stochastic Gradient Descent\n",
    "\n",
    "Advice:\n",
    "    - For deployments - stick with public frameworks!\n",
    "    - Join the Differential Privacy Community\n",
    "    - Don't get ahead of yourself - DP is still in the early days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Project:\n",
    "\n",
    "For the final project for this section, you're going to train a DP model using this PATE method on the MNIST dataset, provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the neccesary modules and datasets.\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "from torch import nn, optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define transforms for the images\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, ), (0.5, ))])\n",
    "# Download the training set.\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the private records out of the large training set.\n",
    "'''\n",
    "Note: This function does not account for splitting of numbers that are not well rounded.\n",
    "'''\n",
    "def create_private_records(trainset, num_hospitals=100):\n",
    "    # Number of records per hospital\n",
    "    batch_size = len(trainset)//num_hospitals\n",
    "    \n",
    "    # Array to store the private records\n",
    "    private_records = []\n",
    "    \n",
    "    # Loop through the train set labels set and split them accordingly.\n",
    "    for starting_point in range(num_hospitals):\n",
    "        start, end = starting_point*batch_size, (1+starting_point)*batch_size\n",
    "        new_private_trainset = Subset(trainset, list(range(start, end)))\n",
    "        loader = DataLoader(new_private_trainset, shuffle=True, batch_size=32, num_workers=2)\n",
    "        private_records.append(loader)\n",
    "        \n",
    "    return private_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear model that will be used in training our model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Add the dropout layers\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train models\n",
    "def train_models(trainloader):\n",
    "    model = Classifier()\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    \n",
    "    # Define the training epochs for each private hospital\n",
    "    epochs = 15\n",
    "    \n",
    "    # Training loop\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Get the log probabilities\n",
    "            log_ps = model(images)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(log_ps, labels)\n",
    "            # Adjust params\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(f\"Training loss: {running_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the records\n",
    "private_records = create_private_records(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training model 1 -----------------------------------------------------------------------\n",
      "Training loss: 42.25011718273163\n",
      "Training loss: 28.17133140563965\n",
      "Training loss: 20.49824494123459\n",
      "Training loss: 16.762129545211792\n",
      "Training loss: 12.677154421806335\n",
      "Training loss: 10.102964997291565\n",
      "Training loss: 8.999305129051208\n",
      "Training loss: 6.614186123013496\n",
      "Training loss: 6.883486121892929\n",
      "Training loss: 6.875231355428696\n",
      "Training loss: 5.675505816936493\n",
      "Training loss: 4.931889995932579\n",
      "Training loss: 5.501518800854683\n",
      "Training loss: 3.91543872654438\n",
      "Training loss: 3.0848151594400406\n",
      "Done training model 1 -----------------------------------------------------------------------\n",
      "Now training model 2 -----------------------------------------------------------------------\n",
      "Training loss: 42.965447425842285\n",
      "Training loss: 33.04808247089386\n",
      "Training loss: 23.2125945687294\n",
      "Training loss: 18.119360327720642\n",
      "Training loss: 15.764095962047577\n",
      "Training loss: 13.607557505369186\n",
      "Training loss: 11.591592520475388\n",
      "Training loss: 9.48633474111557\n",
      "Training loss: 8.634714752435684\n",
      "Training loss: 8.765891708433628\n",
      "Training loss: 6.347307428717613\n",
      "Training loss: 7.508795842528343\n",
      "Training loss: 6.105717293918133\n",
      "Training loss: 6.230396956205368\n",
      "Training loss: 7.701007790863514\n",
      "Done training model 2 -----------------------------------------------------------------------\n",
      "Now training model 3 -----------------------------------------------------------------------\n",
      "Training loss: 42.24276661872864\n",
      "Training loss: 30.034082412719727\n",
      "Training loss: 20.6053187251091\n",
      "Training loss: 14.91420066356659\n",
      "Training loss: 12.772338807582855\n",
      "Training loss: 10.35408341884613\n",
      "Training loss: 9.609351426362991\n",
      "Training loss: 7.731397047638893\n",
      "Training loss: 7.496922329068184\n",
      "Training loss: 6.711999669671059\n",
      "Training loss: 6.0386353731155396\n",
      "Training loss: 5.025139302015305\n",
      "Training loss: 4.193452678620815\n",
      "Training loss: 5.173628859221935\n",
      "Training loss: 5.044288456439972\n",
      "Done training model 3 -----------------------------------------------------------------------\n",
      "Now training model 4 -----------------------------------------------------------------------\n",
      "Training loss: 40.822378516197205\n",
      "Training loss: 26.35547149181366\n",
      "Training loss: 17.77704882621765\n",
      "Training loss: 13.536404132843018\n",
      "Training loss: 11.977392137050629\n",
      "Training loss: 9.422893166542053\n",
      "Training loss: 9.30848079919815\n",
      "Training loss: 6.853522032499313\n",
      "Training loss: 5.828734800219536\n",
      "Training loss: 4.897877261042595\n",
      "Training loss: 5.757149294018745\n",
      "Training loss: 4.678342878818512\n",
      "Training loss: 4.998126573860645\n",
      "Training loss: 4.068825177848339\n",
      "Training loss: 3.3002647385001183\n",
      "Done training model 4 -----------------------------------------------------------------------\n",
      "Now training model 5 -----------------------------------------------------------------------\n",
      "Training loss: 40.9933078289032\n",
      "Training loss: 27.345566153526306\n",
      "Training loss: 20.159102380275726\n",
      "Training loss: 14.651655793190002\n",
      "Training loss: 13.85026103258133\n",
      "Training loss: 10.697155326604843\n",
      "Training loss: 9.07720223069191\n",
      "Training loss: 9.456311792135239\n",
      "Training loss: 7.567817211151123\n",
      "Training loss: 8.27366453409195\n",
      "Training loss: 4.930296614766121\n",
      "Training loss: 4.918639726936817\n",
      "Training loss: 5.083031691610813\n",
      "Training loss: 5.270418308675289\n",
      "Training loss: 3.2504751160740852\n",
      "Done training model 5 -----------------------------------------------------------------------\n",
      "Now training model 6 -----------------------------------------------------------------------\n",
      "Training loss: 43.17809700965881\n",
      "Training loss: 34.226672887802124\n",
      "Training loss: 25.705376505851746\n",
      "Training loss: 19.848431169986725\n",
      "Training loss: 14.762775897979736\n",
      "Training loss: 12.751864403486252\n",
      "Training loss: 9.951794743537903\n",
      "Training loss: 8.347561031579971\n",
      "Training loss: 7.103836938738823\n",
      "Training loss: 6.514978334307671\n",
      "Training loss: 6.996592879295349\n",
      "Training loss: 6.395051524043083\n",
      "Training loss: 6.8230970948934555\n",
      "Training loss: 4.112838946282864\n",
      "Training loss: 3.886826738715172\n",
      "Done training model 6 -----------------------------------------------------------------------\n",
      "Now training model 7 -----------------------------------------------------------------------\n",
      "Training loss: 42.059581995010376\n",
      "Training loss: 30.58346664905548\n",
      "Training loss: 21.628696620464325\n",
      "Training loss: 15.9450364112854\n",
      "Training loss: 11.316031113266945\n",
      "Training loss: 9.556003749370575\n",
      "Training loss: 9.671547695994377\n",
      "Training loss: 8.518983542919159\n",
      "Training loss: 7.151941940188408\n",
      "Training loss: 5.887068346142769\n",
      "Training loss: 5.204780764877796\n",
      "Training loss: 4.933927237987518\n",
      "Training loss: 4.937057517468929\n",
      "Training loss: 4.076158709824085\n",
      "Training loss: 3.222605884075165\n",
      "Done training model 7 -----------------------------------------------------------------------\n",
      "Now training model 8 -----------------------------------------------------------------------\n",
      "Training loss: 42.87951946258545\n",
      "Training loss: 30.174219727516174\n",
      "Training loss: 19.438996255397797\n",
      "Training loss: 13.727121502161026\n",
      "Training loss: 11.735844567418098\n",
      "Training loss: 9.80706898868084\n",
      "Training loss: 7.441238895058632\n",
      "Training loss: 7.511123336851597\n",
      "Training loss: 6.536689966917038\n",
      "Training loss: 6.4520034193992615\n",
      "Training loss: 7.106237635016441\n",
      "Training loss: 5.296177305281162\n",
      "Training loss: 5.320807069540024\n",
      "Training loss: 3.354466326534748\n",
      "Training loss: 4.284751236438751\n",
      "Done training model 8 -----------------------------------------------------------------------\n",
      "Now training model 9 -----------------------------------------------------------------------\n",
      "Training loss: 42.35546147823334\n",
      "Training loss: 32.10027086734772\n",
      "Training loss: 21.2865993976593\n",
      "Training loss: 17.464849650859833\n",
      "Training loss: 14.197900354862213\n",
      "Training loss: 12.663057059049606\n",
      "Training loss: 10.807266384363174\n",
      "Training loss: 8.29387666285038\n",
      "Training loss: 7.34229826182127\n",
      "Training loss: 6.487387850880623\n",
      "Training loss: 7.0474575608968735\n",
      "Training loss: 6.449587181210518\n",
      "Training loss: 5.162975080311298\n",
      "Training loss: 4.407217234373093\n",
      "Training loss: 4.51228379458189\n",
      "Done training model 9 -----------------------------------------------------------------------\n",
      "Now training model 10 -----------------------------------------------------------------------\n",
      "Training loss: 43.039888858795166\n",
      "Training loss: 30.325382351875305\n",
      "Training loss: 21.680168986320496\n",
      "Training loss: 15.744547873735428\n",
      "Training loss: 11.339190870523453\n",
      "Training loss: 10.954326063394547\n",
      "Training loss: 9.149008810520172\n",
      "Training loss: 7.029889330267906\n",
      "Training loss: 6.372064746916294\n",
      "Training loss: 4.241243198513985\n",
      "Training loss: 5.174494281411171\n",
      "Training loss: 4.891748495399952\n",
      "Training loss: 4.701714247465134\n",
      "Training loss: 4.579279959201813\n",
      "Training loss: 4.021106772124767\n",
      "Done training model 10 -----------------------------------------------------------------------\n",
      "Now training model 11 -----------------------------------------------------------------------\n",
      "Training loss: 42.67190718650818\n",
      "Training loss: 28.103074371814728\n",
      "Training loss: 18.70260399580002\n",
      "Training loss: 12.419066667556763\n",
      "Training loss: 9.926593869924545\n",
      "Training loss: 8.930341228842735\n",
      "Training loss: 7.637138321995735\n",
      "Training loss: 6.578264743089676\n",
      "Training loss: 5.560436323285103\n",
      "Training loss: 4.527884311974049\n",
      "Training loss: 3.386944457888603\n",
      "Training loss: 4.313623733818531\n",
      "Training loss: 3.3774875476956367\n",
      "Training loss: 4.088620036840439\n",
      "Training loss: 3.237463168799877\n",
      "Done training model 11 -----------------------------------------------------------------------\n",
      "Now training model 12 -----------------------------------------------------------------------\n",
      "Training loss: 42.46913456916809\n",
      "Training loss: 31.616705417633057\n",
      "Training loss: 22.327936947345734\n",
      "Training loss: 18.314058244228363\n",
      "Training loss: 14.961437791585922\n",
      "Training loss: 11.605067640542984\n",
      "Training loss: 10.674867123365402\n",
      "Training loss: 8.41467933356762\n",
      "Training loss: 7.989937275648117\n",
      "Training loss: 6.7763116508722305\n",
      "Training loss: 7.686318829655647\n",
      "Training loss: 6.559909716248512\n",
      "Training loss: 5.701442703604698\n",
      "Training loss: 5.647344782948494\n",
      "Training loss: 4.621857136487961\n",
      "Done training model 12 -----------------------------------------------------------------------\n",
      "Now training model 13 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 43.616602182388306\n",
      "Training loss: 35.7024462223053\n",
      "Training loss: 24.820565223693848\n",
      "Training loss: 17.656190097332\n",
      "Training loss: 15.01109755039215\n",
      "Training loss: 13.794148087501526\n",
      "Training loss: 12.569796293973923\n",
      "Training loss: 10.793483048677444\n",
      "Training loss: 9.140865325927734\n",
      "Training loss: 8.006233893334866\n",
      "Training loss: 7.555850476026535\n",
      "Training loss: 6.081633307039738\n",
      "Training loss: 6.041595287621021\n",
      "Training loss: 4.440903447568417\n",
      "Training loss: 5.932722836732864\n",
      "Done training model 13 -----------------------------------------------------------------------\n",
      "Now training model 14 -----------------------------------------------------------------------\n",
      "Training loss: 41.66624903678894\n",
      "Training loss: 32.87300658226013\n",
      "Training loss: 22.805163860321045\n",
      "Training loss: 16.20195895433426\n",
      "Training loss: 13.707846760749817\n",
      "Training loss: 11.497107177972794\n",
      "Training loss: 9.654059872031212\n",
      "Training loss: 8.962701164186\n",
      "Training loss: 7.479710295796394\n",
      "Training loss: 7.9583547711372375\n",
      "Training loss: 6.599864691495895\n",
      "Training loss: 5.6894156858325005\n",
      "Training loss: 5.306266441941261\n",
      "Training loss: 5.250068850815296\n",
      "Training loss: 4.116737172007561\n",
      "Done training model 14 -----------------------------------------------------------------------\n",
      "Now training model 15 -----------------------------------------------------------------------\n",
      "Training loss: 42.638880252838135\n",
      "Training loss: 32.851765394210815\n",
      "Training loss: 22.681995570659637\n",
      "Training loss: 18.292827785015106\n",
      "Training loss: 16.313434839248657\n",
      "Training loss: 13.293747305870056\n",
      "Training loss: 12.724114328622818\n",
      "Training loss: 12.103359252214432\n",
      "Training loss: 9.839884132146835\n",
      "Training loss: 7.9876337721943855\n",
      "Training loss: 7.419162519276142\n",
      "Training loss: 7.439609944820404\n",
      "Training loss: 9.115618526935577\n",
      "Training loss: 7.075214296579361\n",
      "Training loss: 6.032669737935066\n",
      "Done training model 15 -----------------------------------------------------------------------\n",
      "Now training model 16 -----------------------------------------------------------------------\n",
      "Training loss: 41.696197390556335\n",
      "Training loss: 28.532999634742737\n",
      "Training loss: 19.629581570625305\n",
      "Training loss: 16.615385949611664\n",
      "Training loss: 12.410735040903091\n",
      "Training loss: 9.634935706853867\n",
      "Training loss: 10.162686884403229\n",
      "Training loss: 9.065018221735954\n",
      "Training loss: 6.926946222782135\n",
      "Training loss: 6.128478463739157\n",
      "Training loss: 5.014571934938431\n",
      "Training loss: 6.597151182591915\n",
      "Training loss: 5.075376965105534\n",
      "Training loss: 4.667391434311867\n",
      "Training loss: 4.750929340720177\n",
      "Done training model 16 -----------------------------------------------------------------------\n",
      "Now training model 17 -----------------------------------------------------------------------\n",
      "Training loss: 41.25303661823273\n",
      "Training loss: 26.36950194835663\n",
      "Training loss: 18.178022861480713\n",
      "Training loss: 15.344986975193024\n",
      "Training loss: 11.472471356391907\n",
      "Training loss: 9.830673664808273\n",
      "Training loss: 9.910726308822632\n",
      "Training loss: 7.273721516132355\n",
      "Training loss: 6.693173408508301\n",
      "Training loss: 5.373365364968777\n",
      "Training loss: 4.420601442456245\n",
      "Training loss: 4.546469412744045\n",
      "Training loss: 3.915323719382286\n",
      "Training loss: 4.608979865908623\n",
      "Training loss: 4.9014757797122\n",
      "Done training model 17 -----------------------------------------------------------------------\n",
      "Now training model 18 -----------------------------------------------------------------------\n",
      "Training loss: 41.71963679790497\n",
      "Training loss: 27.482524514198303\n",
      "Training loss: 18.89016819000244\n",
      "Training loss: 14.273476004600525\n",
      "Training loss: 11.120257467031479\n",
      "Training loss: 10.173248678445816\n",
      "Training loss: 8.419462770223618\n",
      "Training loss: 8.138172596693039\n",
      "Training loss: 5.364015642553568\n",
      "Training loss: 4.9786044880747795\n",
      "Training loss: 5.304985463619232\n",
      "Training loss: 3.7933338806033134\n",
      "Training loss: 3.043940830975771\n",
      "Training loss: 2.9108655005693436\n",
      "Training loss: 2.90806420519948\n",
      "Done training model 18 -----------------------------------------------------------------------\n",
      "Now training model 19 -----------------------------------------------------------------------\n",
      "Training loss: 41.28532540798187\n",
      "Training loss: 26.7159144282341\n",
      "Training loss: 17.79859459400177\n",
      "Training loss: 13.353859543800354\n",
      "Training loss: 9.942447006702423\n",
      "Training loss: 8.821856021881104\n",
      "Training loss: 7.37398162484169\n",
      "Training loss: 7.738651603460312\n",
      "Training loss: 7.383800163865089\n",
      "Training loss: 7.360141173005104\n",
      "Training loss: 4.801555439829826\n",
      "Training loss: 3.324655055999756\n",
      "Training loss: 3.3498109877109528\n",
      "Training loss: 3.3197031021118164\n",
      "Training loss: 2.745987370610237\n",
      "Done training model 19 -----------------------------------------------------------------------\n",
      "Now training model 20 -----------------------------------------------------------------------\n",
      "Training loss: 42.33860385417938\n",
      "Training loss: 31.016038298606873\n",
      "Training loss: 23.570845782756805\n",
      "Training loss: 16.546835362911224\n",
      "Training loss: 14.097201585769653\n",
      "Training loss: 11.18948745727539\n",
      "Training loss: 8.975479781627655\n",
      "Training loss: 7.905505478382111\n",
      "Training loss: 8.065352857112885\n",
      "Training loss: 7.984294816851616\n",
      "Training loss: 6.497779235243797\n",
      "Training loss: 6.03895278275013\n",
      "Training loss: 5.180780656635761\n",
      "Training loss: 6.076375916600227\n",
      "Training loss: 4.796953521668911\n",
      "Done training model 20 -----------------------------------------------------------------------\n",
      "Now training model 21 -----------------------------------------------------------------------\n",
      "Training loss: 41.408528089523315\n",
      "Training loss: 26.536767303943634\n",
      "Training loss: 17.679292142391205\n",
      "Training loss: 12.590988129377365\n",
      "Training loss: 11.299745053052902\n",
      "Training loss: 8.381004050374031\n",
      "Training loss: 6.508506990969181\n",
      "Training loss: 7.2507748901844025\n",
      "Training loss: 6.521460831165314\n",
      "Training loss: 5.2469377517700195\n",
      "Training loss: 4.974371865391731\n",
      "Training loss: 4.646216087043285\n",
      "Training loss: 4.6018839329481125\n",
      "Training loss: 6.019952110946178\n",
      "Training loss: 3.9639891386032104\n",
      "Done training model 21 -----------------------------------------------------------------------\n",
      "Now training model 22 -----------------------------------------------------------------------\n",
      "Training loss: 43.115086793899536\n",
      "Training loss: 35.12120604515076\n",
      "Training loss: 24.34953933954239\n",
      "Training loss: 19.15628159046173\n",
      "Training loss: 16.729872167110443\n",
      "Training loss: 13.728938966989517\n",
      "Training loss: 11.363883227109909\n",
      "Training loss: 10.07827353477478\n",
      "Training loss: 10.721132636070251\n",
      "Training loss: 8.242377802729607\n",
      "Training loss: 7.5431006997823715\n",
      "Training loss: 6.457358554005623\n",
      "Training loss: 5.236136630177498\n",
      "Training loss: 7.74206368625164\n",
      "Training loss: 6.552171766757965\n",
      "Done training model 22 -----------------------------------------------------------------------\n",
      "Now training model 23 -----------------------------------------------------------------------\n",
      "Training loss: 42.872660636901855\n",
      "Training loss: 31.98603892326355\n",
      "Training loss: 21.618797838687897\n",
      "Training loss: 15.213118344545364\n",
      "Training loss: 12.516102194786072\n",
      "Training loss: 10.266986101865768\n",
      "Training loss: 8.41025197505951\n",
      "Training loss: 7.374625027179718\n",
      "Training loss: 6.489816635847092\n",
      "Training loss: 6.817322477698326\n",
      "Training loss: 5.755840636789799\n",
      "Training loss: 6.262432307004929\n",
      "Training loss: 4.246854983270168\n",
      "Training loss: 4.819214574992657\n",
      "Training loss: 4.44243086129427\n",
      "Done training model 23 -----------------------------------------------------------------------\n",
      "Now training model 24 -----------------------------------------------------------------------\n",
      "Training loss: 43.0744891166687\n",
      "Training loss: 33.177215695381165\n",
      "Training loss: 23.46032440662384\n",
      "Training loss: 21.015872716903687\n",
      "Training loss: 16.73286747932434\n",
      "Training loss: 12.354036718606949\n",
      "Training loss: 11.320835381746292\n",
      "Training loss: 9.701361998915672\n",
      "Training loss: 8.124776601791382\n",
      "Training loss: 7.589822396636009\n",
      "Training loss: 6.348916947841644\n",
      "Training loss: 5.792435176670551\n",
      "Training loss: 5.432527542114258\n",
      "Training loss: 5.582557372748852\n",
      "Training loss: 4.381156392395496\n",
      "Done training model 24 -----------------------------------------------------------------------\n",
      "Now training model 25 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.95234775543213\n",
      "Training loss: 32.507383584976196\n",
      "Training loss: 23.07919353246689\n",
      "Training loss: 18.475394248962402\n",
      "Training loss: 13.098932564258575\n",
      "Training loss: 11.585833758115768\n",
      "Training loss: 9.537448406219482\n",
      "Training loss: 8.841892406344414\n",
      "Training loss: 7.7868930250406265\n",
      "Training loss: 8.782415270805359\n",
      "Training loss: 6.4025037214159966\n",
      "Training loss: 6.108853630721569\n",
      "Training loss: 4.170940801501274\n",
      "Training loss: 4.269395411014557\n",
      "Training loss: 4.978101812303066\n",
      "Done training model 25 -----------------------------------------------------------------------\n",
      "Now training model 26 -----------------------------------------------------------------------\n",
      "Training loss: 42.67152142524719\n",
      "Training loss: 29.014799535274506\n",
      "Training loss: 19.05317199230194\n",
      "Training loss: 16.934044897556305\n",
      "Training loss: 12.051654815673828\n",
      "Training loss: 9.829746216535568\n",
      "Training loss: 8.524598836898804\n",
      "Training loss: 8.297035336494446\n",
      "Training loss: 9.08299095928669\n",
      "Training loss: 7.4415344297885895\n",
      "Training loss: 6.129264160990715\n",
      "Training loss: 5.23376852273941\n",
      "Training loss: 4.51699348911643\n",
      "Training loss: 4.396415315568447\n",
      "Training loss: 4.389556303620338\n",
      "Done training model 26 -----------------------------------------------------------------------\n",
      "Now training model 27 -----------------------------------------------------------------------\n",
      "Training loss: 42.89070105552673\n",
      "Training loss: 33.37685310840607\n",
      "Training loss: 23.156551003456116\n",
      "Training loss: 19.065140664577484\n",
      "Training loss: 15.41687786579132\n",
      "Training loss: 14.937555521726608\n",
      "Training loss: 12.10153877735138\n",
      "Training loss: 10.86055313050747\n",
      "Training loss: 8.432289198040962\n",
      "Training loss: 7.9863952696323395\n",
      "Training loss: 6.9453679621219635\n",
      "Training loss: 8.595712438225746\n",
      "Training loss: 5.047546990215778\n",
      "Training loss: 4.678236484527588\n",
      "Training loss: 4.507005639374256\n",
      "Done training model 27 -----------------------------------------------------------------------\n",
      "Now training model 28 -----------------------------------------------------------------------\n",
      "Training loss: 41.90081536769867\n",
      "Training loss: 29.162233114242554\n",
      "Training loss: 19.48830759525299\n",
      "Training loss: 14.47111040353775\n",
      "Training loss: 11.091928750276566\n",
      "Training loss: 9.218690276145935\n",
      "Training loss: 9.11209262907505\n",
      "Training loss: 9.508967384696007\n",
      "Training loss: 7.239403128623962\n",
      "Training loss: 5.281653121113777\n",
      "Training loss: 5.374187670648098\n",
      "Training loss: 6.51584093272686\n",
      "Training loss: 6.1162422969937325\n",
      "Training loss: 4.699250925332308\n",
      "Training loss: 4.543028756976128\n",
      "Done training model 28 -----------------------------------------------------------------------\n",
      "Now training model 29 -----------------------------------------------------------------------\n",
      "Training loss: 43.026655435562134\n",
      "Training loss: 36.17244267463684\n",
      "Training loss: 25.055228412151337\n",
      "Training loss: 17.777671933174133\n",
      "Training loss: 15.11248704791069\n",
      "Training loss: 12.64211955666542\n",
      "Training loss: 10.913450211286545\n",
      "Training loss: 9.213666960597038\n",
      "Training loss: 8.690667644143105\n",
      "Training loss: 7.858211532235146\n",
      "Training loss: 5.621179699897766\n",
      "Training loss: 6.081163041293621\n",
      "Training loss: 5.394005984067917\n",
      "Training loss: 5.276118747889996\n",
      "Training loss: 2.9886036068201065\n",
      "Done training model 29 -----------------------------------------------------------------------\n",
      "Now training model 30 -----------------------------------------------------------------------\n",
      "Training loss: 43.18207859992981\n",
      "Training loss: 34.50083351135254\n",
      "Training loss: 24.541717529296875\n",
      "Training loss: 18.28201949596405\n",
      "Training loss: 15.734106719493866\n",
      "Training loss: 13.172204494476318\n",
      "Training loss: 11.514973878860474\n",
      "Training loss: 10.117965698242188\n",
      "Training loss: 8.66815222799778\n",
      "Training loss: 8.794121205806732\n",
      "Training loss: 7.654924720525742\n",
      "Training loss: 6.333748742938042\n",
      "Training loss: 5.089168660342693\n",
      "Training loss: 6.587836496531963\n",
      "Training loss: 7.028662219643593\n",
      "Done training model 30 -----------------------------------------------------------------------\n",
      "Now training model 31 -----------------------------------------------------------------------\n",
      "Training loss: 42.26936411857605\n",
      "Training loss: 31.865673303604126\n",
      "Training loss: 18.4880610704422\n",
      "Training loss: 13.088198602199554\n",
      "Training loss: 11.205357074737549\n",
      "Training loss: 10.80853283405304\n",
      "Training loss: 7.452059134840965\n",
      "Training loss: 8.333875432610512\n",
      "Training loss: 5.880580917000771\n",
      "Training loss: 5.396983355283737\n",
      "Training loss: 5.0537112802267075\n",
      "Training loss: 4.517022781074047\n",
      "Training loss: 4.976418033242226\n",
      "Training loss: 3.4839952187612653\n",
      "Training loss: 3.0127787329256535\n",
      "Done training model 31 -----------------------------------------------------------------------\n",
      "Now training model 32 -----------------------------------------------------------------------\n",
      "Training loss: 42.50891077518463\n",
      "Training loss: 30.215920329093933\n",
      "Training loss: 21.059917330741882\n",
      "Training loss: 15.099007368087769\n",
      "Training loss: 11.583659321069717\n",
      "Training loss: 9.508497148752213\n",
      "Training loss: 10.23889173567295\n",
      "Training loss: 7.757189720869064\n",
      "Training loss: 6.189851768314838\n",
      "Training loss: 6.022260531783104\n",
      "Training loss: 4.492248483002186\n",
      "Training loss: 3.800995346158743\n",
      "Training loss: 3.9192950651049614\n",
      "Training loss: 4.247068963944912\n",
      "Training loss: 4.891328759491444\n",
      "Done training model 32 -----------------------------------------------------------------------\n",
      "Now training model 33 -----------------------------------------------------------------------\n",
      "Training loss: 41.74594044685364\n",
      "Training loss: 26.766450107097626\n",
      "Training loss: 17.401866853237152\n",
      "Training loss: 12.342927947640419\n",
      "Training loss: 11.353725105524063\n",
      "Training loss: 9.254829198122025\n",
      "Training loss: 7.267643973231316\n",
      "Training loss: 5.955060720443726\n",
      "Training loss: 5.130393169820309\n",
      "Training loss: 4.120874792337418\n",
      "Training loss: 4.454355552792549\n",
      "Training loss: 4.246700033545494\n",
      "Training loss: 5.211181491613388\n",
      "Training loss: 4.664521731436253\n",
      "Training loss: 3.480023741722107\n",
      "Done training model 33 -----------------------------------------------------------------------\n",
      "Now training model 34 -----------------------------------------------------------------------\n",
      "Training loss: 42.82959342002869\n",
      "Training loss: 33.3203786611557\n",
      "Training loss: 23.731346786022186\n",
      "Training loss: 17.190590500831604\n",
      "Training loss: 13.621123611927032\n",
      "Training loss: 11.866741627454758\n",
      "Training loss: 10.393954247236252\n",
      "Training loss: 9.851014196872711\n",
      "Training loss: 7.72794109582901\n",
      "Training loss: 6.362903758883476\n",
      "Training loss: 5.194604158401489\n",
      "Training loss: 4.648813433945179\n",
      "Training loss: 5.064519725739956\n",
      "Training loss: 4.097146958112717\n",
      "Training loss: 3.8837424218654633\n",
      "Done training model 34 -----------------------------------------------------------------------\n",
      "Now training model 35 -----------------------------------------------------------------------\n",
      "Training loss: 43.1220600605011\n",
      "Training loss: 32.42581617832184\n",
      "Training loss: 21.06973034143448\n",
      "Training loss: 15.646941781044006\n",
      "Training loss: 12.570876896381378\n",
      "Training loss: 12.776556491851807\n",
      "Training loss: 9.178002744913101\n",
      "Training loss: 8.267690882086754\n",
      "Training loss: 7.482318595051765\n",
      "Training loss: 6.392351299524307\n",
      "Training loss: 6.022136755287647\n",
      "Training loss: 5.311872750520706\n",
      "Training loss: 3.8325608372688293\n",
      "Training loss: 5.595922484993935\n",
      "Training loss: 5.172153182327747\n",
      "Done training model 35 -----------------------------------------------------------------------\n",
      "Now training model 36 -----------------------------------------------------------------------\n",
      "Training loss: 41.320982933044434\n",
      "Training loss: 30.104214549064636\n",
      "Training loss: 20.70200827717781\n",
      "Training loss: 15.629642605781555\n",
      "Training loss: 11.875616133213043\n",
      "Training loss: 9.94842067360878\n",
      "Training loss: 8.147878125309944\n",
      "Training loss: 6.663412824273109\n",
      "Training loss: 6.242479175329208\n",
      "Training loss: 5.853816136717796\n",
      "Training loss: 3.886148490011692\n",
      "Training loss: 5.492668010294437\n",
      "Training loss: 5.342925518751144\n",
      "Training loss: 3.716541901230812\n",
      "Training loss: 5.412104614078999\n",
      "Done training model 36 -----------------------------------------------------------------------\n",
      "Now training model 37 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 41.218825936317444\n",
      "Training loss: 27.242728650569916\n",
      "Training loss: 18.753988802433014\n",
      "Training loss: 11.99689331650734\n",
      "Training loss: 7.964719727635384\n",
      "Training loss: 9.228682160377502\n",
      "Training loss: 6.877345606684685\n",
      "Training loss: 6.011553831398487\n",
      "Training loss: 4.436461433768272\n",
      "Training loss: 4.398379094898701\n",
      "Training loss: 3.8592583313584328\n",
      "Training loss: 3.900221072137356\n",
      "Training loss: 4.470794394612312\n",
      "Training loss: 4.20293915271759\n",
      "Training loss: 3.1585925072431564\n",
      "Done training model 37 -----------------------------------------------------------------------\n",
      "Now training model 38 -----------------------------------------------------------------------\n",
      "Training loss: 41.496949672698975\n",
      "Training loss: 28.441283106803894\n",
      "Training loss: 19.914393365383148\n",
      "Training loss: 14.369397938251495\n",
      "Training loss: 11.944374024868011\n",
      "Training loss: 10.258343636989594\n",
      "Training loss: 9.148418635129929\n",
      "Training loss: 8.602932527661324\n",
      "Training loss: 8.173945501446724\n",
      "Training loss: 8.23881022632122\n",
      "Training loss: 6.272066056728363\n",
      "Training loss: 4.957855924963951\n",
      "Training loss: 4.272075023502111\n",
      "Training loss: 4.6954729706048965\n",
      "Training loss: 3.4579501897096634\n",
      "Done training model 38 -----------------------------------------------------------------------\n",
      "Now training model 39 -----------------------------------------------------------------------\n",
      "Training loss: 40.69376313686371\n",
      "Training loss: 26.14900428056717\n",
      "Training loss: 15.833681523799896\n",
      "Training loss: 13.109360218048096\n",
      "Training loss: 10.611851423978806\n",
      "Training loss: 8.301616936922073\n",
      "Training loss: 7.856604114174843\n",
      "Training loss: 6.092257350683212\n",
      "Training loss: 4.9914411418139935\n",
      "Training loss: 4.920262768864632\n",
      "Training loss: 5.273346059024334\n",
      "Training loss: 4.183454714715481\n",
      "Training loss: 3.577516309916973\n",
      "Training loss: 5.232233338057995\n",
      "Training loss: 4.5264244973659515\n",
      "Done training model 39 -----------------------------------------------------------------------\n",
      "Now training model 40 -----------------------------------------------------------------------\n",
      "Training loss: 40.88641548156738\n",
      "Training loss: 27.340554893016815\n",
      "Training loss: 19.867172241210938\n",
      "Training loss: 17.46181058883667\n",
      "Training loss: 13.453264892101288\n",
      "Training loss: 10.10468327999115\n",
      "Training loss: 9.801833748817444\n",
      "Training loss: 8.238811582326889\n",
      "Training loss: 7.409320995211601\n",
      "Training loss: 6.928900763392448\n",
      "Training loss: 6.659115627408028\n",
      "Training loss: 4.83230596780777\n",
      "Training loss: 5.669728830456734\n",
      "Training loss: 5.173809051513672\n",
      "Training loss: 4.700933538377285\n",
      "Done training model 40 -----------------------------------------------------------------------\n",
      "Now training model 41 -----------------------------------------------------------------------\n",
      "Training loss: 41.177045464515686\n",
      "Training loss: 26.3217111825943\n",
      "Training loss: 18.481551826000214\n",
      "Training loss: 13.382633954286575\n",
      "Training loss: 12.10553327202797\n",
      "Training loss: 10.129049152135849\n",
      "Training loss: 9.352265864610672\n",
      "Training loss: 9.171481877565384\n",
      "Training loss: 8.802458345890045\n",
      "Training loss: 7.722284883260727\n",
      "Training loss: 6.651497431099415\n",
      "Training loss: 4.864587314426899\n",
      "Training loss: 4.375451914966106\n",
      "Training loss: 4.111350581049919\n",
      "Training loss: 4.82234963029623\n",
      "Done training model 41 -----------------------------------------------------------------------\n",
      "Now training model 42 -----------------------------------------------------------------------\n",
      "Training loss: 43.25725269317627\n",
      "Training loss: 33.8559844493866\n",
      "Training loss: 23.288131177425385\n",
      "Training loss: 16.027798891067505\n",
      "Training loss: 13.414208173751831\n",
      "Training loss: 10.237501308321953\n",
      "Training loss: 10.206714630126953\n",
      "Training loss: 8.622686371207237\n",
      "Training loss: 7.017359286546707\n",
      "Training loss: 6.112689897418022\n",
      "Training loss: 6.892378896474838\n",
      "Training loss: 4.8931387439370155\n",
      "Training loss: 5.339071311056614\n",
      "Training loss: 6.193557113409042\n",
      "Training loss: 5.703537747263908\n",
      "Done training model 42 -----------------------------------------------------------------------\n",
      "Now training model 43 -----------------------------------------------------------------------\n",
      "Training loss: 41.75787818431854\n",
      "Training loss: 27.237273275852203\n",
      "Training loss: 17.132213681936264\n",
      "Training loss: 14.493474066257477\n",
      "Training loss: 11.288599461317062\n",
      "Training loss: 10.420011788606644\n",
      "Training loss: 9.191409811377525\n",
      "Training loss: 7.2239982932806015\n",
      "Training loss: 6.341364488005638\n",
      "Training loss: 5.468930967152119\n",
      "Training loss: 5.770217306911945\n",
      "Training loss: 4.4980225041508675\n",
      "Training loss: 4.86843541264534\n",
      "Training loss: 4.064490519464016\n",
      "Training loss: 3.904401145875454\n",
      "Done training model 43 -----------------------------------------------------------------------\n",
      "Now training model 44 -----------------------------------------------------------------------\n",
      "Training loss: 43.35327768325806\n",
      "Training loss: 31.483028650283813\n",
      "Training loss: 21.195240199565887\n",
      "Training loss: 16.576826959848404\n",
      "Training loss: 13.487203985452652\n",
      "Training loss: 10.0437550842762\n",
      "Training loss: 8.585687667131424\n",
      "Training loss: 8.472899794578552\n",
      "Training loss: 8.31617283821106\n",
      "Training loss: 7.641704306006432\n",
      "Training loss: 6.08296811580658\n",
      "Training loss: 4.7027817517519\n",
      "Training loss: 5.725666299462318\n",
      "Training loss: 4.933145426213741\n",
      "Training loss: 4.321802847087383\n",
      "Done training model 44 -----------------------------------------------------------------------\n",
      "Now training model 45 -----------------------------------------------------------------------\n",
      "Training loss: 43.16486072540283\n",
      "Training loss: 35.161996364593506\n",
      "Training loss: 25.528258562088013\n",
      "Training loss: 19.545651137828827\n",
      "Training loss: 17.175304770469666\n",
      "Training loss: 15.889201879501343\n",
      "Training loss: 13.39647325873375\n",
      "Training loss: 10.817905694246292\n",
      "Training loss: 10.339651197195053\n",
      "Training loss: 9.026128441095352\n",
      "Training loss: 7.538807511329651\n",
      "Training loss: 7.254386126995087\n",
      "Training loss: 7.322351202368736\n",
      "Training loss: 7.571535117924213\n",
      "Training loss: 6.248398721218109\n",
      "Done training model 45 -----------------------------------------------------------------------\n",
      "Now training model 46 -----------------------------------------------------------------------\n",
      "Training loss: 41.99169456958771\n",
      "Training loss: 28.473606169223785\n",
      "Training loss: 21.036854028701782\n",
      "Training loss: 16.087280809879303\n",
      "Training loss: 12.881133735179901\n",
      "Training loss: 9.612501069903374\n",
      "Training loss: 9.922982767224312\n",
      "Training loss: 7.168209932744503\n",
      "Training loss: 5.823195569217205\n",
      "Training loss: 5.227076090872288\n",
      "Training loss: 6.356304377317429\n",
      "Training loss: 4.461832918226719\n",
      "Training loss: 4.777100794017315\n",
      "Training loss: 3.91870429366827\n",
      "Training loss: 3.350166417658329\n",
      "Done training model 46 -----------------------------------------------------------------------\n",
      "Now training model 47 -----------------------------------------------------------------------\n",
      "Training loss: 41.68222987651825\n",
      "Training loss: 28.117474734783173\n",
      "Training loss: 20.92100429534912\n",
      "Training loss: 16.056725203990936\n",
      "Training loss: 11.877773404121399\n",
      "Training loss: 10.731024950742722\n",
      "Training loss: 8.256335973739624\n",
      "Training loss: 7.418081060051918\n",
      "Training loss: 6.402245059609413\n",
      "Training loss: 5.772087313234806\n",
      "Training loss: 7.437329486012459\n",
      "Training loss: 4.396566636860371\n",
      "Training loss: 4.146268554031849\n",
      "Training loss: 3.752097934484482\n",
      "Training loss: 3.807669647037983\n",
      "Done training model 47 -----------------------------------------------------------------------\n",
      "Now training model 48 -----------------------------------------------------------------------\n",
      "Training loss: 40.99736249446869\n",
      "Training loss: 27.49910056591034\n",
      "Training loss: 19.830027878284454\n",
      "Training loss: 15.074593424797058\n",
      "Training loss: 10.848837673664093\n",
      "Training loss: 9.447516724467278\n",
      "Training loss: 9.53830872476101\n",
      "Training loss: 6.859886139631271\n",
      "Training loss: 7.115274176001549\n",
      "Training loss: 6.261931926012039\n",
      "Training loss: 5.428431987762451\n",
      "Training loss: 6.009764134883881\n",
      "Training loss: 3.8902312517166138\n",
      "Training loss: 4.346808589994907\n",
      "Training loss: 3.584820508956909\n",
      "Done training model 48 -----------------------------------------------------------------------\n",
      "Now training model 49 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 43.43012976646423\n",
      "Training loss: 36.17626488208771\n",
      "Training loss: 25.63556182384491\n",
      "Training loss: 19.569786429405212\n",
      "Training loss: 16.373845100402832\n",
      "Training loss: 13.393934220075607\n",
      "Training loss: 11.371892720460892\n",
      "Training loss: 10.61538016796112\n",
      "Training loss: 9.581541895866394\n",
      "Training loss: 7.559187278151512\n",
      "Training loss: 6.747621938586235\n",
      "Training loss: 4.685275614261627\n",
      "Training loss: 5.107784062623978\n",
      "Training loss: 5.085374400019646\n",
      "Training loss: 5.393282353878021\n",
      "Done training model 49 -----------------------------------------------------------------------\n",
      "Now training model 50 -----------------------------------------------------------------------\n",
      "Training loss: 42.45743250846863\n",
      "Training loss: 32.46400773525238\n",
      "Training loss: 23.60301297903061\n",
      "Training loss: 20.248558938503265\n",
      "Training loss: 15.448853611946106\n",
      "Training loss: 14.49923861026764\n",
      "Training loss: 13.041747152805328\n",
      "Training loss: 9.843846648931503\n",
      "Training loss: 10.633634805679321\n",
      "Training loss: 7.963269367814064\n",
      "Training loss: 7.478344917297363\n",
      "Training loss: 5.911666393280029\n",
      "Training loss: 5.770993642508984\n",
      "Training loss: 5.378399513661861\n",
      "Training loss: 6.327522709965706\n",
      "Done training model 50 -----------------------------------------------------------------------\n",
      "Now training model 51 -----------------------------------------------------------------------\n",
      "Training loss: 43.01102018356323\n",
      "Training loss: 34.3466215133667\n",
      "Training loss: 24.551516890525818\n",
      "Training loss: 18.958973109722137\n",
      "Training loss: 14.814111709594727\n",
      "Training loss: 12.538878828287125\n",
      "Training loss: 10.705614000558853\n",
      "Training loss: 8.734716951847076\n",
      "Training loss: 7.557720057666302\n",
      "Training loss: 6.694412991404533\n",
      "Training loss: 7.904762640595436\n",
      "Training loss: 7.059894502162933\n",
      "Training loss: 8.271389991044998\n",
      "Training loss: 6.535654172301292\n",
      "Training loss: 5.92830578237772\n",
      "Done training model 51 -----------------------------------------------------------------------\n",
      "Now training model 52 -----------------------------------------------------------------------\n",
      "Training loss: 42.49655532836914\n",
      "Training loss: 31.795148372650146\n",
      "Training loss: 24.61998099088669\n",
      "Training loss: 18.46672874689102\n",
      "Training loss: 14.815986633300781\n",
      "Training loss: 12.457079261541367\n",
      "Training loss: 10.725657984614372\n",
      "Training loss: 9.841244727373123\n",
      "Training loss: 8.128675177693367\n",
      "Training loss: 8.231391556560993\n",
      "Training loss: 7.617064356803894\n",
      "Training loss: 6.104745775461197\n",
      "Training loss: 5.275439955294132\n",
      "Training loss: 5.119162127375603\n",
      "Training loss: 6.467671640217304\n",
      "Done training model 52 -----------------------------------------------------------------------\n",
      "Now training model 53 -----------------------------------------------------------------------\n",
      "Training loss: 43.32077646255493\n",
      "Training loss: 36.03888666629791\n",
      "Training loss: 26.467349529266357\n",
      "Training loss: 19.349190264940262\n",
      "Training loss: 17.126558005809784\n",
      "Training loss: 14.11602509021759\n",
      "Training loss: 11.366759568452835\n",
      "Training loss: 11.40658164024353\n",
      "Training loss: 9.08866560459137\n",
      "Training loss: 8.649728164076805\n",
      "Training loss: 7.856271535158157\n",
      "Training loss: 6.124443978071213\n",
      "Training loss: 6.657411076128483\n",
      "Training loss: 5.884441830217838\n",
      "Training loss: 6.068655222654343\n",
      "Done training model 53 -----------------------------------------------------------------------\n",
      "Now training model 54 -----------------------------------------------------------------------\n",
      "Training loss: 43.17468214035034\n",
      "Training loss: 35.94661998748779\n",
      "Training loss: 25.31726610660553\n",
      "Training loss: 20.056535840034485\n",
      "Training loss: 14.555597186088562\n",
      "Training loss: 11.283941000699997\n",
      "Training loss: 10.879378110170364\n",
      "Training loss: 8.65849819779396\n",
      "Training loss: 7.33059561252594\n",
      "Training loss: 7.631660416722298\n",
      "Training loss: 7.387813150882721\n",
      "Training loss: 6.693591400980949\n",
      "Training loss: 5.123408921062946\n",
      "Training loss: 5.833668828010559\n",
      "Training loss: 5.126720927655697\n",
      "Done training model 54 -----------------------------------------------------------------------\n",
      "Now training model 55 -----------------------------------------------------------------------\n",
      "Training loss: 42.85281038284302\n",
      "Training loss: 31.3515545129776\n",
      "Training loss: 21.544739544391632\n",
      "Training loss: 16.600964605808258\n",
      "Training loss: 13.818121254444122\n",
      "Training loss: 12.17423266172409\n",
      "Training loss: 8.754149362444878\n",
      "Training loss: 7.912517726421356\n",
      "Training loss: 8.403155013918877\n",
      "Training loss: 7.2570312693715096\n",
      "Training loss: 7.001778334379196\n",
      "Training loss: 6.724962018430233\n",
      "Training loss: 5.219162184745073\n",
      "Training loss: 4.057874083518982\n",
      "Training loss: 4.819766938686371\n",
      "Done training model 55 -----------------------------------------------------------------------\n",
      "Now training model 56 -----------------------------------------------------------------------\n",
      "Training loss: 42.07906150817871\n",
      "Training loss: 30.490938663482666\n",
      "Training loss: 19.3655446767807\n",
      "Training loss: 15.835398375988007\n",
      "Training loss: 12.392294317483902\n",
      "Training loss: 11.660933762788773\n",
      "Training loss: 8.958239912986755\n",
      "Training loss: 8.336947709321976\n",
      "Training loss: 5.989439904689789\n",
      "Training loss: 4.848158545792103\n",
      "Training loss: 6.018043048679829\n",
      "Training loss: 4.623460009694099\n",
      "Training loss: 5.383032776415348\n",
      "Training loss: 5.246055461466312\n",
      "Training loss: 4.345392793416977\n",
      "Done training model 56 -----------------------------------------------------------------------\n",
      "Now training model 57 -----------------------------------------------------------------------\n",
      "Training loss: 41.385279297828674\n",
      "Training loss: 29.516594767570496\n",
      "Training loss: 20.95271497964859\n",
      "Training loss: 14.946748793125153\n",
      "Training loss: 12.560943871736526\n",
      "Training loss: 10.941838473081589\n",
      "Training loss: 8.479612678289413\n",
      "Training loss: 7.825582295656204\n",
      "Training loss: 5.970987841486931\n",
      "Training loss: 5.267135299742222\n",
      "Training loss: 5.620004586875439\n",
      "Training loss: 4.583882950246334\n",
      "Training loss: 8.109471440315247\n",
      "Training loss: 5.9309979528188705\n",
      "Training loss: 6.189748451113701\n",
      "Done training model 57 -----------------------------------------------------------------------\n",
      "Now training model 58 -----------------------------------------------------------------------\n",
      "Training loss: 42.36262786388397\n",
      "Training loss: 30.90314209461212\n",
      "Training loss: 19.340920627117157\n",
      "Training loss: 14.339986383914948\n",
      "Training loss: 12.365787625312805\n",
      "Training loss: 11.177568107843399\n",
      "Training loss: 9.503911763429642\n",
      "Training loss: 9.297213852405548\n",
      "Training loss: 7.481526643037796\n",
      "Training loss: 6.478164106607437\n",
      "Training loss: 6.597196385264397\n",
      "Training loss: 5.773983187973499\n",
      "Training loss: 4.882859632372856\n",
      "Training loss: 4.251046486198902\n",
      "Training loss: 4.724859468638897\n",
      "Done training model 58 -----------------------------------------------------------------------\n",
      "Now training model 59 -----------------------------------------------------------------------\n",
      "Training loss: 42.511582136154175\n",
      "Training loss: 30.61101734638214\n",
      "Training loss: 20.802144169807434\n",
      "Training loss: 15.659112870693207\n",
      "Training loss: 12.655469387769699\n",
      "Training loss: 10.719773650169373\n",
      "Training loss: 9.059913203120232\n",
      "Training loss: 6.777864441275597\n",
      "Training loss: 8.71244202554226\n",
      "Training loss: 6.000307708978653\n",
      "Training loss: 6.027877375483513\n",
      "Training loss: 6.334644258022308\n",
      "Training loss: 5.1943189650774\n",
      "Training loss: 4.191010802984238\n",
      "Training loss: 5.5076135620474815\n",
      "Done training model 59 -----------------------------------------------------------------------\n",
      "Now training model 60 -----------------------------------------------------------------------\n",
      "Training loss: 43.200923681259155\n",
      "Training loss: 33.88166356086731\n",
      "Training loss: 23.122479557991028\n",
      "Training loss: 15.824368834495544\n",
      "Training loss: 13.48997288942337\n",
      "Training loss: 11.009736180305481\n",
      "Training loss: 10.552981108427048\n",
      "Training loss: 8.20347948372364\n",
      "Training loss: 8.599752500653267\n",
      "Training loss: 6.946508452296257\n",
      "Training loss: 7.045067206025124\n",
      "Training loss: 5.160175941884518\n",
      "Training loss: 4.292746663093567\n",
      "Training loss: 6.429442144930363\n",
      "Training loss: 5.313188381493092\n",
      "Done training model 60 -----------------------------------------------------------------------\n",
      "Now training model 61 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.38548481464386\n",
      "Training loss: 28.741788744926453\n",
      "Training loss: 18.098626971244812\n",
      "Training loss: 14.288932770490646\n",
      "Training loss: 11.155569970607758\n",
      "Training loss: 10.511412531137466\n",
      "Training loss: 8.46380278468132\n",
      "Training loss: 8.110654383897781\n",
      "Training loss: 6.790456056594849\n",
      "Training loss: 7.022439636290073\n",
      "Training loss: 5.494554094970226\n",
      "Training loss: 5.786411575973034\n",
      "Training loss: 4.82923574000597\n",
      "Training loss: 3.2090227231383324\n",
      "Training loss: 3.928903251886368\n",
      "Done training model 61 -----------------------------------------------------------------------\n",
      "Now training model 62 -----------------------------------------------------------------------\n",
      "Training loss: 42.73391914367676\n",
      "Training loss: 31.04487919807434\n",
      "Training loss: 19.022476136684418\n",
      "Training loss: 16.92832440137863\n",
      "Training loss: 12.641871392726898\n",
      "Training loss: 11.316271424293518\n",
      "Training loss: 9.508503273129463\n",
      "Training loss: 8.075926542282104\n",
      "Training loss: 6.218617528676987\n",
      "Training loss: 6.592641927301884\n",
      "Training loss: 5.629233203828335\n",
      "Training loss: 5.303562700748444\n",
      "Training loss: 5.403679095208645\n",
      "Training loss: 4.950430914759636\n",
      "Training loss: 3.356283277273178\n",
      "Done training model 62 -----------------------------------------------------------------------\n",
      "Now training model 63 -----------------------------------------------------------------------\n",
      "Training loss: 42.85337471961975\n",
      "Training loss: 31.33101737499237\n",
      "Training loss: 19.486062705516815\n",
      "Training loss: 16.245018124580383\n",
      "Training loss: 13.942303955554962\n",
      "Training loss: 11.04277491569519\n",
      "Training loss: 9.872855737805367\n",
      "Training loss: 9.829746961593628\n",
      "Training loss: 8.357381939888\n",
      "Training loss: 6.380756571888924\n",
      "Training loss: 7.700360208749771\n",
      "Training loss: 5.882062017917633\n",
      "Training loss: 5.045610696077347\n",
      "Training loss: 5.120118126273155\n",
      "Training loss: 3.75568887591362\n",
      "Done training model 63 -----------------------------------------------------------------------\n",
      "Now training model 64 -----------------------------------------------------------------------\n",
      "Training loss: 42.39069616794586\n",
      "Training loss: 30.99945306777954\n",
      "Training loss: 20.925891041755676\n",
      "Training loss: 17.63631170988083\n",
      "Training loss: 14.498977363109589\n",
      "Training loss: 12.403564125299454\n",
      "Training loss: 10.585146754980087\n",
      "Training loss: 9.59345543384552\n",
      "Training loss: 8.779807209968567\n",
      "Training loss: 7.58257719874382\n",
      "Training loss: 7.9931580722332\n",
      "Training loss: 5.97518664598465\n",
      "Training loss: 5.906173340976238\n",
      "Training loss: 5.882963985204697\n",
      "Training loss: 5.719940580427647\n",
      "Done training model 64 -----------------------------------------------------------------------\n",
      "Now training model 65 -----------------------------------------------------------------------\n",
      "Training loss: 41.847119212150574\n",
      "Training loss: 29.301373958587646\n",
      "Training loss: 19.13791912794113\n",
      "Training loss: 16.309064149856567\n",
      "Training loss: 11.643083989620209\n",
      "Training loss: 9.067383199930191\n",
      "Training loss: 7.805889055132866\n",
      "Training loss: 6.7430766969919205\n",
      "Training loss: 5.399433150887489\n",
      "Training loss: 5.833020269870758\n",
      "Training loss: 4.614738993346691\n",
      "Training loss: 4.809926696121693\n",
      "Training loss: 5.51884301751852\n",
      "Training loss: 5.022499546408653\n",
      "Training loss: 3.717018701136112\n",
      "Done training model 65 -----------------------------------------------------------------------\n",
      "Now training model 66 -----------------------------------------------------------------------\n",
      "Training loss: 41.14854598045349\n",
      "Training loss: 27.65221858024597\n",
      "Training loss: 19.597759127616882\n",
      "Training loss: 15.4123295545578\n",
      "Training loss: 12.510323971509933\n",
      "Training loss: 11.600374847650528\n",
      "Training loss: 10.15289306640625\n",
      "Training loss: 8.875099018216133\n",
      "Training loss: 7.820465385913849\n",
      "Training loss: 6.162032321095467\n",
      "Training loss: 5.962343856692314\n",
      "Training loss: 6.180777609348297\n",
      "Training loss: 5.546794049441814\n",
      "Training loss: 5.467695444822311\n",
      "Training loss: 4.111904688179493\n",
      "Done training model 66 -----------------------------------------------------------------------\n",
      "Now training model 67 -----------------------------------------------------------------------\n",
      "Training loss: 42.68079900741577\n",
      "Training loss: 31.52568769454956\n",
      "Training loss: 25.019111931324005\n",
      "Training loss: 18.477210819721222\n",
      "Training loss: 15.374785602092743\n",
      "Training loss: 12.074653178453445\n",
      "Training loss: 10.293334022164345\n",
      "Training loss: 8.235695853829384\n",
      "Training loss: 8.425501525402069\n",
      "Training loss: 6.921647518873215\n",
      "Training loss: 7.270652800798416\n",
      "Training loss: 5.685980699956417\n",
      "Training loss: 5.5540371015667915\n",
      "Training loss: 5.171721555292606\n",
      "Training loss: 4.8101048693060875\n",
      "Done training model 67 -----------------------------------------------------------------------\n",
      "Now training model 68 -----------------------------------------------------------------------\n",
      "Training loss: 42.37315893173218\n",
      "Training loss: 30.22943890094757\n",
      "Training loss: 20.94312620162964\n",
      "Training loss: 15.549079418182373\n",
      "Training loss: 10.124282658100128\n",
      "Training loss: 8.802717685699463\n",
      "Training loss: 9.649239212274551\n",
      "Training loss: 7.521273270249367\n",
      "Training loss: 6.316668041050434\n",
      "Training loss: 6.055785529315472\n",
      "Training loss: 6.025244399905205\n",
      "Training loss: 5.063116788864136\n",
      "Training loss: 4.802351653575897\n",
      "Training loss: 3.939568266272545\n",
      "Training loss: 3.1152475103735924\n",
      "Done training model 68 -----------------------------------------------------------------------\n",
      "Now training model 69 -----------------------------------------------------------------------\n",
      "Training loss: 43.61184883117676\n",
      "Training loss: 36.10114109516144\n",
      "Training loss: 24.5451899766922\n",
      "Training loss: 18.062099933624268\n",
      "Training loss: 14.34432789683342\n",
      "Training loss: 11.445873975753784\n",
      "Training loss: 10.339602530002594\n",
      "Training loss: 11.442172795534134\n",
      "Training loss: 9.072051465511322\n",
      "Training loss: 6.363538317382336\n",
      "Training loss: 6.395351082086563\n",
      "Training loss: 6.019668273627758\n",
      "Training loss: 5.287119150161743\n",
      "Training loss: 4.813469871878624\n",
      "Training loss: 5.284213006496429\n",
      "Done training model 69 -----------------------------------------------------------------------\n",
      "Now training model 70 -----------------------------------------------------------------------\n",
      "Training loss: 42.70316934585571\n",
      "Training loss: 34.58432483673096\n",
      "Training loss: 24.085551142692566\n",
      "Training loss: 19.06379473209381\n",
      "Training loss: 14.694941461086273\n",
      "Training loss: 13.031062364578247\n",
      "Training loss: 9.699539631605148\n",
      "Training loss: 8.90179118514061\n",
      "Training loss: 6.605959072709084\n",
      "Training loss: 7.126937881112099\n",
      "Training loss: 8.833223909139633\n",
      "Training loss: 7.006033673882484\n",
      "Training loss: 7.0156260803341866\n",
      "Training loss: 6.253715738654137\n",
      "Training loss: 5.139861516654491\n",
      "Done training model 70 -----------------------------------------------------------------------\n",
      "Now training model 71 -----------------------------------------------------------------------\n",
      "Training loss: 42.287341475486755\n",
      "Training loss: 30.226076006889343\n",
      "Training loss: 21.08491438627243\n",
      "Training loss: 16.336169838905334\n",
      "Training loss: 13.929943442344666\n",
      "Training loss: 11.234165042638779\n",
      "Training loss: 9.622367292642593\n",
      "Training loss: 8.557121008634567\n",
      "Training loss: 8.469687819480896\n",
      "Training loss: 6.83419668674469\n",
      "Training loss: 6.404358632862568\n",
      "Training loss: 5.6192052364349365\n",
      "Training loss: 6.143561139702797\n",
      "Training loss: 4.570599988102913\n",
      "Training loss: 4.4934848845005035\n",
      "Done training model 71 -----------------------------------------------------------------------\n",
      "Now training model 72 -----------------------------------------------------------------------\n",
      "Training loss: 42.88848280906677\n",
      "Training loss: 32.630122661590576\n",
      "Training loss: 21.514404714107513\n",
      "Training loss: 16.67884960770607\n",
      "Training loss: 13.9866943359375\n",
      "Training loss: 11.260158509016037\n",
      "Training loss: 9.280877754092216\n",
      "Training loss: 8.178435623645782\n",
      "Training loss: 6.381738916039467\n",
      "Training loss: 6.998919725418091\n",
      "Training loss: 6.978138297796249\n",
      "Training loss: 5.3889066353440285\n",
      "Training loss: 6.38119250535965\n",
      "Training loss: 6.426530763506889\n",
      "Training loss: 3.978543423116207\n",
      "Done training model 72 -----------------------------------------------------------------------\n",
      "Now training model 73 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.59602427482605\n",
      "Training loss: 29.318235993385315\n",
      "Training loss: 19.35516995191574\n",
      "Training loss: 13.929045915603638\n",
      "Training loss: 11.628582268953323\n",
      "Training loss: 9.96645411849022\n",
      "Training loss: 8.605683028697968\n",
      "Training loss: 7.169203571975231\n",
      "Training loss: 6.256303861737251\n",
      "Training loss: 6.642523467540741\n",
      "Training loss: 5.126345582306385\n",
      "Training loss: 3.912801019847393\n",
      "Training loss: 4.222969397902489\n",
      "Training loss: 4.396644577383995\n",
      "Training loss: 4.24478243291378\n",
      "Done training model 73 -----------------------------------------------------------------------\n",
      "Now training model 74 -----------------------------------------------------------------------\n",
      "Training loss: 42.5792293548584\n",
      "Training loss: 33.18204963207245\n",
      "Training loss: 22.226775884628296\n",
      "Training loss: 17.3982612490654\n",
      "Training loss: 13.68740138411522\n",
      "Training loss: 12.17437794804573\n",
      "Training loss: 9.896014422178268\n",
      "Training loss: 9.053712010383606\n",
      "Training loss: 7.836619593203068\n",
      "Training loss: 7.504547148942947\n",
      "Training loss: 7.0993556678295135\n",
      "Training loss: 6.582431450486183\n",
      "Training loss: 4.076020002365112\n",
      "Training loss: 4.265348546206951\n",
      "Training loss: 4.634441904723644\n",
      "Done training model 74 -----------------------------------------------------------------------\n",
      "Now training model 75 -----------------------------------------------------------------------\n",
      "Training loss: 43.06193518638611\n",
      "Training loss: 32.07748281955719\n",
      "Training loss: 21.99935072660446\n",
      "Training loss: 16.441696643829346\n",
      "Training loss: 13.30351847410202\n",
      "Training loss: 10.7865591943264\n",
      "Training loss: 10.388788104057312\n",
      "Training loss: 9.33705721795559\n",
      "Training loss: 7.4217003881931305\n",
      "Training loss: 7.230444759130478\n",
      "Training loss: 6.120008319616318\n",
      "Training loss: 5.294299386441708\n",
      "Training loss: 4.513302840292454\n",
      "Training loss: 3.724086806178093\n",
      "Training loss: 5.015113785862923\n",
      "Done training model 75 -----------------------------------------------------------------------\n",
      "Now training model 76 -----------------------------------------------------------------------\n",
      "Training loss: 40.68064546585083\n",
      "Training loss: 27.235157668590546\n",
      "Training loss: 20.572380542755127\n",
      "Training loss: 18.532967746257782\n",
      "Training loss: 12.80580073595047\n",
      "Training loss: 12.01415878534317\n",
      "Training loss: 8.922293901443481\n",
      "Training loss: 8.625142246484756\n",
      "Training loss: 7.332081511616707\n",
      "Training loss: 6.379835397005081\n",
      "Training loss: 5.244077369570732\n",
      "Training loss: 7.743984252214432\n",
      "Training loss: 4.64972297847271\n",
      "Training loss: 4.906923942267895\n",
      "Training loss: 5.461579285562038\n",
      "Done training model 76 -----------------------------------------------------------------------\n",
      "Now training model 77 -----------------------------------------------------------------------\n",
      "Training loss: 42.86224400997162\n",
      "Training loss: 32.54095256328583\n",
      "Training loss: 24.36303436756134\n",
      "Training loss: 19.05100965499878\n",
      "Training loss: 14.928219556808472\n",
      "Training loss: 13.758364707231522\n",
      "Training loss: 11.615902423858643\n",
      "Training loss: 9.490929961204529\n",
      "Training loss: 8.626423701643944\n",
      "Training loss: 9.750200808048248\n",
      "Training loss: 7.023863896727562\n",
      "Training loss: 5.8995334059000015\n",
      "Training loss: 6.634248822927475\n",
      "Training loss: 5.791122488677502\n",
      "Training loss: 5.705225169658661\n",
      "Done training model 77 -----------------------------------------------------------------------\n",
      "Now training model 78 -----------------------------------------------------------------------\n",
      "Training loss: 41.13570821285248\n",
      "Training loss: 28.64902687072754\n",
      "Training loss: 19.376854836940765\n",
      "Training loss: 16.49590575695038\n",
      "Training loss: 12.420373976230621\n",
      "Training loss: 11.571988940238953\n",
      "Training loss: 10.205425262451172\n",
      "Training loss: 8.234911948442459\n",
      "Training loss: 6.84379930049181\n",
      "Training loss: 6.9970259964466095\n",
      "Training loss: 6.4664295464754105\n",
      "Training loss: 6.248512536287308\n",
      "Training loss: 5.479209065437317\n",
      "Training loss: 4.569648794829845\n",
      "Training loss: 4.352296486496925\n",
      "Done training model 78 -----------------------------------------------------------------------\n",
      "Now training model 79 -----------------------------------------------------------------------\n",
      "Training loss: 42.63385009765625\n",
      "Training loss: 31.412597179412842\n",
      "Training loss: 21.352131009101868\n",
      "Training loss: 17.62449187040329\n",
      "Training loss: 15.742260336875916\n",
      "Training loss: 12.487618446350098\n",
      "Training loss: 9.156919807195663\n",
      "Training loss: 8.211106643080711\n",
      "Training loss: 7.035070613026619\n",
      "Training loss: 7.346669897437096\n",
      "Training loss: 7.462801963090897\n",
      "Training loss: 6.100852899253368\n",
      "Training loss: 5.362725295126438\n",
      "Training loss: 5.636194549500942\n",
      "Training loss: 4.1038186475634575\n",
      "Done training model 79 -----------------------------------------------------------------------\n",
      "Now training model 80 -----------------------------------------------------------------------\n",
      "Training loss: 41.82804024219513\n",
      "Training loss: 30.808526396751404\n",
      "Training loss: 20.893027544021606\n",
      "Training loss: 15.773219525814056\n",
      "Training loss: 13.17083889245987\n",
      "Training loss: 11.147002443671227\n",
      "Training loss: 10.531496316194534\n",
      "Training loss: 10.214271038770676\n",
      "Training loss: 7.823017969727516\n",
      "Training loss: 5.508294478058815\n",
      "Training loss: 7.787513427436352\n",
      "Training loss: 8.514697223901749\n",
      "Training loss: 5.533836267888546\n",
      "Training loss: 4.5585388988256454\n",
      "Training loss: 5.464964598417282\n",
      "Done training model 80 -----------------------------------------------------------------------\n",
      "Now training model 81 -----------------------------------------------------------------------\n",
      "Training loss: 40.98756194114685\n",
      "Training loss: 26.26750761270523\n",
      "Training loss: 17.142599642276764\n",
      "Training loss: 14.680635333061218\n",
      "Training loss: 9.792368292808533\n",
      "Training loss: 8.839266449213028\n",
      "Training loss: 8.979874342679977\n",
      "Training loss: 7.327784091234207\n",
      "Training loss: 7.22410124540329\n",
      "Training loss: 4.709360301494598\n",
      "Training loss: 5.2747251614928246\n",
      "Training loss: 4.5790990218520164\n",
      "Training loss: 3.8605999127030373\n",
      "Training loss: 3.1031741350889206\n",
      "Training loss: 4.4719505831599236\n",
      "Done training model 81 -----------------------------------------------------------------------\n",
      "Now training model 82 -----------------------------------------------------------------------\n",
      "Training loss: 43.67139768600464\n",
      "Training loss: 34.191096782684326\n",
      "Training loss: 23.065494775772095\n",
      "Training loss: 18.68317025899887\n",
      "Training loss: 15.27282303571701\n",
      "Training loss: 12.792569696903229\n",
      "Training loss: 11.114912033081055\n",
      "Training loss: 9.080930426716805\n",
      "Training loss: 8.512416779994965\n",
      "Training loss: 6.73530401289463\n",
      "Training loss: 5.5372385531663895\n",
      "Training loss: 7.888847976922989\n",
      "Training loss: 5.939147770404816\n",
      "Training loss: 6.1601599752902985\n",
      "Training loss: 4.868552275002003\n",
      "Done training model 82 -----------------------------------------------------------------------\n",
      "Now training model 83 -----------------------------------------------------------------------\n",
      "Training loss: 43.49932837486267\n",
      "Training loss: 34.944859862327576\n",
      "Training loss: 26.275701761245728\n",
      "Training loss: 20.77700638771057\n",
      "Training loss: 15.156464159488678\n",
      "Training loss: 14.051100015640259\n",
      "Training loss: 12.210734248161316\n",
      "Training loss: 9.71272286772728\n",
      "Training loss: 8.966386467218399\n",
      "Training loss: 8.573754414916039\n",
      "Training loss: 6.93287955224514\n",
      "Training loss: 6.69357143342495\n",
      "Training loss: 7.307286933064461\n",
      "Training loss: 6.687151774764061\n",
      "Training loss: 6.090555794537067\n",
      "Done training model 83 -----------------------------------------------------------------------\n",
      "Now training model 84 -----------------------------------------------------------------------\n",
      "Training loss: 42.93860864639282\n",
      "Training loss: 32.69805097579956\n",
      "Training loss: 23.152831196784973\n",
      "Training loss: 18.826820194721222\n",
      "Training loss: 14.233262956142426\n",
      "Training loss: 11.985601633787155\n",
      "Training loss: 11.316427886486053\n",
      "Training loss: 10.654298156499863\n",
      "Training loss: 9.203119337558746\n",
      "Training loss: 6.848206892609596\n",
      "Training loss: 6.795364584773779\n",
      "Training loss: 6.228986166417599\n",
      "Training loss: 5.016932711005211\n",
      "Training loss: 5.103217266499996\n",
      "Training loss: 4.715989485383034\n",
      "Done training model 84 -----------------------------------------------------------------------\n",
      "Now training model 85 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.759623289108276\n",
      "Training loss: 32.000091195106506\n",
      "Training loss: 21.218022108078003\n",
      "Training loss: 16.10883539915085\n",
      "Training loss: 11.900939166545868\n",
      "Training loss: 9.681818574666977\n",
      "Training loss: 8.198317378759384\n",
      "Training loss: 7.754913121461868\n",
      "Training loss: 6.323792412877083\n",
      "Training loss: 6.660492017865181\n",
      "Training loss: 6.352651491761208\n",
      "Training loss: 5.685465082526207\n",
      "Training loss: 4.471103124320507\n",
      "Training loss: 5.400659203529358\n",
      "Training loss: 4.330243483185768\n",
      "Done training model 85 -----------------------------------------------------------------------\n",
      "Now training model 86 -----------------------------------------------------------------------\n",
      "Training loss: 42.40042960643768\n",
      "Training loss: 27.18841540813446\n",
      "Training loss: 16.98110020160675\n",
      "Training loss: 12.643344938755035\n",
      "Training loss: 10.551554024219513\n",
      "Training loss: 9.964377850294113\n",
      "Training loss: 9.12142613530159\n",
      "Training loss: 6.864773407578468\n",
      "Training loss: 6.334258064627647\n",
      "Training loss: 5.144985623657703\n",
      "Training loss: 4.55671077221632\n",
      "Training loss: 3.8999795094132423\n",
      "Training loss: 3.9714419804513454\n",
      "Training loss: 4.206055887043476\n",
      "Training loss: 5.461207889020443\n",
      "Done training model 86 -----------------------------------------------------------------------\n",
      "Now training model 87 -----------------------------------------------------------------------\n",
      "Training loss: 43.29922842979431\n",
      "Training loss: 34.706549286842346\n",
      "Training loss: 24.027952075004578\n",
      "Training loss: 17.77548086643219\n",
      "Training loss: 13.087362706661224\n",
      "Training loss: 12.345601081848145\n",
      "Training loss: 9.817101627588272\n",
      "Training loss: 9.788464352488518\n",
      "Training loss: 9.686461687088013\n",
      "Training loss: 7.8456125035882\n",
      "Training loss: 6.902698516845703\n",
      "Training loss: 5.848314315080643\n",
      "Training loss: 5.759271800518036\n",
      "Training loss: 6.0368786081671715\n",
      "Training loss: 5.1418904438614845\n",
      "Done training model 87 -----------------------------------------------------------------------\n",
      "Now training model 88 -----------------------------------------------------------------------\n",
      "Training loss: 41.84207558631897\n",
      "Training loss: 29.365915775299072\n",
      "Training loss: 19.530993938446045\n",
      "Training loss: 12.530006766319275\n",
      "Training loss: 12.45250654220581\n",
      "Training loss: 11.049610197544098\n",
      "Training loss: 8.712610587477684\n",
      "Training loss: 7.746777221560478\n",
      "Training loss: 6.726980485022068\n",
      "Training loss: 6.669327616691589\n",
      "Training loss: 5.922171264886856\n",
      "Training loss: 4.871256172657013\n",
      "Training loss: 4.457912251353264\n",
      "Training loss: 3.5790344700217247\n",
      "Training loss: 4.648277685046196\n",
      "Done training model 88 -----------------------------------------------------------------------\n",
      "Now training model 89 -----------------------------------------------------------------------\n",
      "Training loss: 42.16456210613251\n",
      "Training loss: 31.096255779266357\n",
      "Training loss: 22.101690649986267\n",
      "Training loss: 16.002366185188293\n",
      "Training loss: 15.156574547290802\n",
      "Training loss: 11.6203673183918\n",
      "Training loss: 8.627117052674294\n",
      "Training loss: 10.093992918729782\n",
      "Training loss: 7.943479686975479\n",
      "Training loss: 6.985145151615143\n",
      "Training loss: 5.892287313938141\n",
      "Training loss: 6.829843387007713\n",
      "Training loss: 5.255190879106522\n",
      "Training loss: 5.886487547308207\n",
      "Training loss: 4.901986204087734\n",
      "Done training model 89 -----------------------------------------------------------------------\n",
      "Now training model 90 -----------------------------------------------------------------------\n",
      "Training loss: 42.39508330821991\n",
      "Training loss: 31.003764867782593\n",
      "Training loss: 21.219296514987946\n",
      "Training loss: 16.141426146030426\n",
      "Training loss: 15.034989804029465\n",
      "Training loss: 11.286016523838043\n",
      "Training loss: 9.79424338042736\n",
      "Training loss: 8.735574916005135\n",
      "Training loss: 7.420635402202606\n",
      "Training loss: 7.601023808121681\n",
      "Training loss: 7.012784078717232\n",
      "Training loss: 5.793119795620441\n",
      "Training loss: 4.975294701755047\n",
      "Training loss: 4.750039160251617\n",
      "Training loss: 4.999964669346809\n",
      "Done training model 90 -----------------------------------------------------------------------\n",
      "Now training model 91 -----------------------------------------------------------------------\n",
      "Training loss: 43.10704517364502\n",
      "Training loss: 31.049469709396362\n",
      "Training loss: 20.8525248169899\n",
      "Training loss: 15.549862444400787\n",
      "Training loss: 14.141929686069489\n",
      "Training loss: 11.012288302183151\n",
      "Training loss: 8.880158618092537\n",
      "Training loss: 8.47757339477539\n",
      "Training loss: 7.633424907922745\n",
      "Training loss: 6.038612835109234\n",
      "Training loss: 6.0478678196668625\n",
      "Training loss: 6.265343993902206\n",
      "Training loss: 4.888318188488483\n",
      "Training loss: 4.892559580504894\n",
      "Training loss: 4.683735616505146\n",
      "Done training model 91 -----------------------------------------------------------------------\n",
      "Now training model 92 -----------------------------------------------------------------------\n",
      "Training loss: 41.89592695236206\n",
      "Training loss: 27.980830550193787\n",
      "Training loss: 18.365289986133575\n",
      "Training loss: 15.706080079078674\n",
      "Training loss: 13.521994948387146\n",
      "Training loss: 10.077606081962585\n",
      "Training loss: 9.68334610760212\n",
      "Training loss: 6.974993795156479\n",
      "Training loss: 6.968053512275219\n",
      "Training loss: 7.079015523195267\n",
      "Training loss: 5.82877261377871\n",
      "Training loss: 5.9943481013178825\n",
      "Training loss: 5.423300854861736\n",
      "Training loss: 3.982385978102684\n",
      "Training loss: 3.7166279330849648\n",
      "Done training model 92 -----------------------------------------------------------------------\n",
      "Now training model 93 -----------------------------------------------------------------------\n",
      "Training loss: 40.348312735557556\n",
      "Training loss: 25.892829298973083\n",
      "Training loss: 16.543922245502472\n",
      "Training loss: 12.445416390895844\n",
      "Training loss: 11.036678731441498\n",
      "Training loss: 8.020181357860565\n",
      "Training loss: 7.20166976749897\n",
      "Training loss: 6.137480147182941\n",
      "Training loss: 6.57270060479641\n",
      "Training loss: 6.090300835669041\n",
      "Training loss: 4.918777480721474\n",
      "Training loss: 3.882509011775255\n",
      "Training loss: 3.5393407195806503\n",
      "Training loss: 3.254910722374916\n",
      "Training loss: 2.7333295196294785\n",
      "Done training model 93 -----------------------------------------------------------------------\n",
      "Now training model 94 -----------------------------------------------------------------------\n",
      "Training loss: 42.06455183029175\n",
      "Training loss: 30.68281662464142\n",
      "Training loss: 19.979144155979156\n",
      "Training loss: 16.191820979118347\n",
      "Training loss: 11.745475828647614\n",
      "Training loss: 10.08687350153923\n",
      "Training loss: 8.17442998290062\n",
      "Training loss: 8.365568697452545\n",
      "Training loss: 6.55736231058836\n",
      "Training loss: 6.489552527666092\n",
      "Training loss: 4.4222307577729225\n",
      "Training loss: 4.970763593912125\n",
      "Training loss: 3.4347559064626694\n",
      "Training loss: 3.4862161688506603\n",
      "Training loss: 5.279573768377304\n",
      "Done training model 94 -----------------------------------------------------------------------\n",
      "Now training model 95 -----------------------------------------------------------------------\n",
      "Training loss: 42.07212793827057\n",
      "Training loss: 29.006091117858887\n",
      "Training loss: 19.429690837860107\n",
      "Training loss: 14.008279979228973\n",
      "Training loss: 13.199449926614761\n",
      "Training loss: 13.050900340080261\n",
      "Training loss: 9.368140578269958\n",
      "Training loss: 8.660523816943169\n",
      "Training loss: 5.8689263463020325\n",
      "Training loss: 6.065350733697414\n",
      "Training loss: 4.283549405634403\n",
      "Training loss: 4.828604236245155\n",
      "Training loss: 4.80044224858284\n",
      "Training loss: 3.322443202137947\n",
      "Training loss: 3.979209464043379\n",
      "Done training model 95 -----------------------------------------------------------------------\n",
      "Now training model 96 -----------------------------------------------------------------------\n",
      "Training loss: 41.7419753074646\n",
      "Training loss: 30.455217123031616\n",
      "Training loss: 19.778206944465637\n",
      "Training loss: 13.800867348909378\n",
      "Training loss: 11.62538143992424\n",
      "Training loss: 10.136166602373123\n",
      "Training loss: 9.059454709291458\n",
      "Training loss: 8.280165046453476\n",
      "Training loss: 6.464216239750385\n",
      "Training loss: 6.266279399394989\n",
      "Training loss: 4.835088014602661\n",
      "Training loss: 3.9738746136426926\n",
      "Training loss: 4.878979347646236\n",
      "Training loss: 5.937815256416798\n",
      "Training loss: 4.011652514338493\n",
      "Done training model 96 -----------------------------------------------------------------------\n",
      "Now training model 97 -----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 42.475022196769714\n",
      "Training loss: 30.029919743537903\n",
      "Training loss: 21.302348017692566\n",
      "Training loss: 14.891701340675354\n",
      "Training loss: 12.499325722455978\n",
      "Training loss: 9.796682834625244\n",
      "Training loss: 7.874158099293709\n",
      "Training loss: 7.2238843739032745\n",
      "Training loss: 7.071487240493298\n",
      "Training loss: 7.256507143378258\n",
      "Training loss: 5.197588451206684\n",
      "Training loss: 3.9700623601675034\n",
      "Training loss: 3.6312388330698013\n",
      "Training loss: 4.48834727704525\n",
      "Training loss: 4.246149353682995\n",
      "Done training model 97 -----------------------------------------------------------------------\n",
      "Now training model 98 -----------------------------------------------------------------------\n",
      "Training loss: 42.29720139503479\n",
      "Training loss: 29.359031438827515\n",
      "Training loss: 17.033787190914154\n",
      "Training loss: 10.298967808485031\n",
      "Training loss: 7.7953204065561295\n",
      "Training loss: 5.387475281953812\n",
      "Training loss: 6.384798943996429\n",
      "Training loss: 4.158434800803661\n",
      "Training loss: 3.4719373136758804\n",
      "Training loss: 3.643278367817402\n",
      "Training loss: 2.0224866941571236\n",
      "Training loss: 1.9688489437103271\n",
      "Training loss: 2.5880354903638363\n",
      "Training loss: 2.5841676145792007\n",
      "Training loss: 2.3462896272540092\n",
      "Done training model 98 -----------------------------------------------------------------------\n",
      "Now training model 99 -----------------------------------------------------------------------\n",
      "Training loss: 40.144598960876465\n",
      "Training loss: 22.589243292808533\n",
      "Training loss: 12.917169988155365\n",
      "Training loss: 9.066987469792366\n",
      "Training loss: 6.782799497246742\n",
      "Training loss: 5.702274918556213\n",
      "Training loss: 4.586298257112503\n",
      "Training loss: 2.7158380299806595\n",
      "Training loss: 2.7005858160555363\n",
      "Training loss: 2.0884059965610504\n",
      "Training loss: 2.211124986410141\n",
      "Training loss: 1.4044074658304453\n",
      "Training loss: 1.1607772558927536\n",
      "Training loss: 1.4817077117040753\n",
      "Training loss: 2.4044268927536905\n",
      "Done training model 99 -----------------------------------------------------------------------\n",
      "Now training model 100 -----------------------------------------------------------------------\n",
      "Training loss: 38.95501732826233\n",
      "Training loss: 20.91412115097046\n",
      "Training loss: 13.657924950122833\n",
      "Training loss: 10.790967583656311\n",
      "Training loss: 8.148483410477638\n",
      "Training loss: 5.797009661793709\n",
      "Training loss: 4.690186560153961\n",
      "Training loss: 4.277095586061478\n",
      "Training loss: 3.9009626135230064\n",
      "Training loss: 5.252789154648781\n",
      "Training loss: 3.7113053053617477\n",
      "Training loss: 3.623031087219715\n",
      "Training loss: 2.4494580030441284\n",
      "Training loss: 2.9685830250382423\n",
      "Training loss: 3.291003217920661\n",
      "Done training model 100 -----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now we want to train all the independent models separately.\n",
    "all_models = []\n",
    "for index, records in enumerate(private_records):\n",
    "    print(\"Now training model {} -----------------------------------------------------------------------\".format(index+1))\n",
    "    model = train_models(records)\n",
    "    all_models.append(model)\n",
    "    print(\"Done training model {} -----------------------------------------------------------------------\".format(index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loader for the test dataset\n",
    "test_loader = DataLoader(testset, shuffle=True, batch_size=1, num_workers=2)\n",
    "\n",
    "# Run each model on all of the test datasets.\n",
    "preds = []\n",
    "for model in all_models:\n",
    "    model_pred = []\n",
    "    for images, _ in test_loader:\n",
    "        ps = torch.exp(model(images))\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        model_pred.append(top_class.item())\n",
    "    preds.append(model_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "preds = np.array(preds, dtype=int)\n",
    "preds = preds.transpose(1,0)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain final Labels\n",
    "new_labels = list()\n",
    "for an_image in preds:\n",
    "\n",
    "    label_counts = np.bincount(an_image, minlength=10)\n",
    "\n",
    "    epsilon = 0.2\n",
    "    beta = 1 / epsilon\n",
    "\n",
    "    for i in range(len(label_counts)):\n",
    "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "\n",
    "    new_label = np.argmax(label_counts)\n",
    "    \n",
    "    new_labels.append(new_label)\n",
    "\n",
    "# Convert new labels into a numpy array\n",
    "new_labels = np.array(new_labels, dtype=int)\n",
    "new_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 1611.5129254649705\n",
      "Data Dependent Epsilon: 1611.5129254651984\n"
     ]
    }
   ],
   "source": [
    "## PATE Analysis\n",
    "preds = preds.transpose(1,0) # Change the shape back to number_of_teachers X number_of_examples\n",
    "# print(preds.shape)\n",
    "\n",
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=new_labels, noise_eps= 0.2, delta=1e-5)\n",
    "# assert data_dep_eps < data_ind_eps\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
